{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "464941a0-c342-4b6b-b3bc-71e7cad3f6eb",
   "metadata": {},
   "source": [
    "### 0. Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb3eac9-69c4-43b2-bf28-09d658012839",
   "metadata": {},
   "source": [
    " - What is embedding?\n",
    " - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2027fc63-e425-4c3a-b45a-e89e567eae6e",
   "metadata": {},
   "source": [
    "### 1. Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2cacc5-f7f3-4082-9979-6679e60a39e5",
   "metadata": {},
   "source": [
    "First, let's import needed modules and, random seed (we'll use it if needed) and create some auxiliary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "581b5387-0b6f-4d93-860e-ef0aba2c7579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31d0be56-84d2-4b3d-9123-0b0b41ff10e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52826a9c-fd97-48f3-ada4-104915ec76da",
   "metadata": {},
   "source": [
    "### 2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dff2098-0287-41b6-ad17-465a98f9ac96",
   "metadata": {},
   "source": [
    "I'll be using dataset from [Spooky Author Identification](https://www.kaggle.com/c/spooky-author-identification/overview) competition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256b5b8b-0ad8-4e21-8f70-ebc9bd317cb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1 Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ffa751c-18dc-4ce4-a34b-32963e704960",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a823ea3-0b17-4fb4-a81b-f79e36e88a59",
   "metadata": {},
   "source": [
    "#### 2.2 Data Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cb1cb5-9541-4db5-ab53-d2eb4a422da8",
   "metadata": {},
   "source": [
    "* id - a unique identifier for each sentence\n",
    "* text - some text written by one of the authors\n",
    "* author - the author of the sentence (EAP: Edgar Allan Poe, HPL: HP Lovecraft; MWS: Mary Wollstonecraft Shelley)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4849f6-e2d4-4ee1-9098-755d7b5f3c74",
   "metadata": {},
   "source": [
    "Let's look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7855d2ae-7f71-4cc1-b5b1-bbe01b63d113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7acaa22-858e-4962-a3d9-361228e0c635",
   "metadata": {},
   "source": [
    "For now, I'm going to look only at column text to look at the ways text representation can be done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff621cc0-8965-4b9d-863a-213b997ff0be",
   "metadata": {},
   "source": [
    "#### 2.3 Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed7ab0b-4174-4728-969c-ceb3635c23c5",
   "metadata": {},
   "source": [
    "But nevertheless let's split the data into training and validation sets.  \n",
    "As soon as we have almost $20 000$ rows in `train_df` test size will be limited to $10\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0154461-4b3a-445e-b439-2930233bc107",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(train_df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2ac83c-4476-4a0c-abb8-047e51f51947",
   "metadata": {},
   "source": [
    "### 3. Text embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20bed68-aa67-4d14-a45a-1b0956409fbd",
   "metadata": {},
   "source": [
    "#### 3.1 Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a612f62b-d324-4892-904d-72d3a3e719ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 3.1.1 One-hot vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632d2a19-43a6-42c3-b6d6-b2ec2b48789f",
   "metadata": {},
   "source": [
    "The simplest way of word representation is **one-hot vectors**. For the i-th word in the vocabulary, the vector has 1 on the i-th dimension and 0 on the rest.   \n",
    "Let's do this using sklearn's `CountVectorizer` with `binary=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b186ea0d-8a8a-4aba-bed2-4dc5059e9699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (17621, 24036)\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(binary=True)\n",
    "X_train_oh = count_vect.fit_transform(train['text'])\n",
    "print(f\"The size of the train dataset is {X_train_cv.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a67895-fc80-4251-b5e5-bd1c3794a88e",
   "metadata": {},
   "source": [
    "By default, we are not limiting the vocabulary of the model and the length of the vector for every sentence will be $24066$ - number of words in our vocab.\n",
    "Although, it can be done by setting parameter `max_features` to, for example, $10000$. By doing this, vocabulary will be built considering only the top `max_features` ordered by term frequency across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b66834b-b43c-41ed-bc27-ad31ef1b29d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<17621x24036 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 386331 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_oh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4346339-6c2b-4a7f-b656-29c62ad0b418",
   "metadata": {},
   "source": [
    "It is also worth to mention, that due to the sparsity of representation (most values in word vectors will be zeros) we can save a lot of memory by only storing the non-zero parts of the feature vectors in memory. `scipy.sparse` matrices are data structures that do exactly this and they are used in `sklearn` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0835806a-fd90-432c-b114-18da4b67c901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "423538356"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "17621*24036"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef0dbef-8b0c-4dad-a68b-84e9ab2c2ef3",
   "metadata": {},
   "source": [
    "In our case only $386$ $331$ of elements out of $423$ $538$ $356$ are non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33cc670b-58d1-42ed-9c8c-dde458d946d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17621, 10000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect_lim_vocab = CountVectorizer(binary=True, max_features=10_000)\n",
    "X_train_oh = count_vect_lim_vocab.fit_transform(train['text'])\n",
    "X_train_oh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0aa8a-cbf5-4065-8149-2ec629598d4c",
   "metadata": {},
   "source": [
    "Now, the length of the vector is $10000$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b62305-0e58-4e9e-b33b-bbb0ef92518b",
   "metadata": {},
   "source": [
    "Let's look at the vector for the first text in our train corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df405b94-371d-46b9-bfa1-d6c68b251174",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence = train['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e24d901b-d0ae-4c31-90a3-1b43e9886f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x24036 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 34 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.transform([first_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37edd75b-1795-46f9-b6b6-a221afb16fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_vector = count_vect.transform([first_sentence]).toarray()\n",
    "one_hot_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6b4454-cba2-4e07-86ad-4ee95a66a4a3",
   "metadata": {},
   "source": [
    "It is a sparse vector with $24066$ elements with only $34$ elements that are not equal to zero. \n",
    "Let's find out which are they."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "163f00ab-8a9a-453a-a896-5595ebf5cbf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  436,   809,  1247,  1260,  1596,  1979,  3579,  5902,  6629,\n",
       "        7902, 10360, 11618, 12856, 13128, 13145, 13371, 13907, 14215,\n",
       "       14529, 14779, 15414, 15950, 16439, 17826, 18751, 18890, 19538,\n",
       "       21188, 21276, 21488, 22367, 23257, 23499, 23722], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.where(np.any(one_hot_vector!=0, axis=0))[0]\n",
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711b8e60-d43a-477c-9a16-178c3e62d33f",
   "metadata": {},
   "source": [
    "These are the indices of words which are present in our sentence.  \n",
    "Now we are going to create index-to-word dictionary to check the result of the work of `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "585e32cb-1ddb-4ef2-b314-2d276461f389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{14301: 'nothing',\n",
       " 4678: 'could',\n",
       " 7595: 'exceed',\n",
       " 21188: 'the',\n",
       " 12650: 'love',\n",
       " 809: 'and',\n",
       " 17728: 'respect',\n",
       " 23517: 'which',\n",
       " 23964: 'younger',\n",
       " 4668: 'cottagers'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word = {index : word for word, index in count_vect.vocabulary_.items()}\n",
    "dict(take(10, index_to_word.items()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1477509-68cf-4784-aeac-b793232d51a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(436, 'afforded', 1),\n",
       " (809, 'and', 1),\n",
       " (1247, 'as', 1),\n",
       " (1260, 'ascertaining', 1),\n",
       " (1596, 'aware', 1),\n",
       " (1979, 'being', 1),\n",
       " (3579, 'circuit', 1),\n",
       " (5902, 'dimensions', 1),\n",
       " (6629, 'dungeon', 1),\n",
       " (7902, 'fact', 1),\n",
       " (10360, 'however', 1),\n",
       " (11618, 'its', 1),\n",
       " (12856, 'make', 1),\n",
       " (13128, 'me', 1),\n",
       " (13145, 'means', 1),\n",
       " (13371, 'might', 1),\n",
       " (13907, 'my', 1),\n",
       " (14215, 'no', 1),\n",
       " (14529, 'of', 1),\n",
       " (14779, 'out', 1),\n",
       " (15414, 'perfectly', 1),\n",
       " (15950, 'point', 1),\n",
       " (16439, 'process', 1),\n",
       " (17826, 'return', 1),\n",
       " (18751, 'seemed', 1),\n",
       " (18890, 'set', 1),\n",
       " (19538, 'so', 1),\n",
       " (21188, 'the', 1),\n",
       " (21276, 'this', 1),\n",
       " (21488, 'to', 1),\n",
       " (22367, 'uniform', 1),\n",
       " (23257, 'wall', 1),\n",
       " (23499, 'whence', 1),\n",
       " (23722, 'without', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(ind, index_to_word[ind], one_hot_vector[0, ind]) for ind in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc193b3-57cf-4829-9424-a0841befb951",
   "metadata": {},
   "source": [
    "Indeed, these are the indices and corresponding words from our sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26827458-5e5f-48cb-99c1-fc2fae032fde",
   "metadata": {},
   "source": [
    "Because we've created the `CountVectorizer` with `binary=True`. The elements are really ones and zeros.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d0a03-a27f-4def-882b-59ca316df9da",
   "metadata": {},
   "source": [
    "##### 3.1.2 One-hot vectors with counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6b28ef-3ba7-40cd-99e4-52d594953c28",
   "metadata": {},
   "source": [
    "A little improvement over that will be using vectorizer with `binary=False`, because this way we will take counts into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c6158ae-5dc7-4ee0-84d1-f91d3bb5f6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(436, 'afforded', 1),\n",
       " (809, 'and', 1),\n",
       " (1247, 'as', 1),\n",
       " (1260, 'ascertaining', 1),\n",
       " (1596, 'aware', 1),\n",
       " (1979, 'being', 1),\n",
       " (3579, 'circuit', 1),\n",
       " (5902, 'dimensions', 1),\n",
       " (6629, 'dungeon', 1),\n",
       " (7902, 'fact', 1),\n",
       " (10360, 'however', 1),\n",
       " (11618, 'its', 1),\n",
       " (12856, 'make', 1),\n",
       " (13128, 'me', 1),\n",
       " (13145, 'means', 1),\n",
       " (13371, 'might', 1),\n",
       " (13907, 'my', 1),\n",
       " (14215, 'no', 1),\n",
       " (14529, 'of', 3),\n",
       " (14779, 'out', 1),\n",
       " (15414, 'perfectly', 1),\n",
       " (15950, 'point', 1),\n",
       " (16439, 'process', 1),\n",
       " (17826, 'return', 1),\n",
       " (18751, 'seemed', 1),\n",
       " (18890, 'set', 1),\n",
       " (19538, 'so', 1),\n",
       " (21188, 'the', 4),\n",
       " (21276, 'this', 1),\n",
       " (21488, 'to', 1),\n",
       " (22367, 'uniform', 1),\n",
       " (23257, 'wall', 1),\n",
       " (23499, 'whence', 1),\n",
       " (23722, 'without', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(binary=False)\n",
    "X_train_cv = count_vect.fit_transform(train['text'])\n",
    "\n",
    "one_hot_vector = count_vect.transform([first_sentence]).toarray()\n",
    "indices = np.where(np.any(one_hot_vector!=0, axis=0))[0]\n",
    "index_to_word = {index : word for word, index in count_vect.vocabulary_.items()}\n",
    "\n",
    "[(ind, index_to_word[ind], one_hot_vector[0, ind]) for ind in indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8553e30c-b37e-44a4-a8ca-b7a2c71ea00a",
   "metadata": {},
   "source": [
    "We can see that now the value for 'the' is 4. It doesn't only show that this article is present in the sentence, but also indicate how many times it occurs in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1201b65-a554-45e3-90bf-95b2e4c5f124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = count_vect.build_tokenizer()\n",
    "tokenized_sentence = tokenizer(first_sentence.lower())\n",
    "tokenized_sentence.count('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c827759a-7dc9-4358-9617-31635a650874",
   "metadata": {},
   "source": [
    "##### 3.1.3 N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00ab815-bb23-42a7-a957-26ef2e5081db",
   "metadata": {},
   "source": [
    "We can take into account not only words, but collocations using parameter `ngram_range`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57503039-5cf5-4a7f-919e-2a9ccf11b8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (17621, 230401)\n",
      "<bound method _data_matrix.count_nonzero of <17621x230401 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 813688 stored elements in Compressed Sparse Row format>>\n",
      "[(2705, 'afforded', 1), (2713, 'afforded me', 1), (7701, 'and', 1), (11504, 'and return', 1), (16642, 'as', 1), (17111, 'as might', 1), (17638, 'ascertaining', 1), (17640, 'ascertaining the', 1), (19859, 'aware', 1), (19863, 'aware of', 1), (24230, 'being', 1), (24255, 'being aware', 1), (36131, 'circuit', 1), (36132, 'circuit and', 1), (50328, 'dimensions', 1), (50336, 'dimensions of', 1), (54676, 'dungeon', 1), (54677, 'dungeon as', 1), (63613, 'fact', 1), (63651, 'fact so', 1), (91067, 'however', 1), (91069, 'however afforded', 1), (101503, 'its', 1), (101621, 'its circuit', 1), (112791, 'make', 1), (112843, 'make its', 1), (115046, 'me', 1), (115371, 'me no', 1), (115739, 'means', 1), (115779, 'means of', 1), (117354, 'might', 1), (117461, 'might make', 1), (122535, 'my', 1), (122933, 'my dungeon', 1), (126846, 'no', 1), (127161, 'no means', 1), (131289, 'of', 3), (131486, 'of ascertaining', 1), (133621, 'of my', 1), (134797, 'of the', 1), (140495, 'out', 1), (140626, 'out without', 1), (144682, 'perfectly', 1), (144713, 'perfectly uniform', 1), (147668, 'point', 1), (147734, 'point whence', 1), (150655, 'process', 1), (150657, 'process however', 1), (158948, 'return', 1), (158991, 'return to', 1), (165075, 'seemed', 1), (165210, 'seemed the', 1), (166300, 'set', 1), (166345, 'set out', 1), (171949, 'so', 1), (172369, 'so perfectly', 1), (186674, 'the', 4), (188442, 'the dimensions', 1), (188976, 'the fact', 1), (191552, 'the point', 1), (193756, 'the wall', 1), (197424, 'this', 1), (198164, 'this process', 1), (201143, 'to', 1), (203049, 'to the', 1), (208316, 'uniform', 1), (208322, 'uniform seemed', 1), (214366, 'wall', 1), (220458, 'whence', 1), (220472, 'whence set', 1), (226046, 'without', 1), (226067, 'without being', 1)]\n"
     ]
    }
   ],
   "source": [
    "bigram_count_vect = CountVectorizer(binary=False, ngram_range=(1,2))\n",
    "X_train_bigram = bigram_count_vect.fit_transform(train['text'])\n",
    "print(f\"The size of the train dataset is {X_train_cv.shape}\")\n",
    "print(X_train_bigram.count_nonzero)\n",
    "one_hot_vector = bigram_count_vect.transform([first_sentence]).toarray()\n",
    "indices = np.where(np.any(one_hot_vector!=0, axis=0))[0]\n",
    "index_to_word = {index : word for word, index in count_vect.vocabulary_.items()}\n",
    "\n",
    "print([(ind, index_to_word[ind], one_hot_vector[0, ind]) for ind in indices])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c8e685-d4dc-4fd3-997a-922dfe4db161",
   "metadata": {},
   "source": [
    "By including bigrams into calculations the size of the vector increased from $24$ $066$ to $230$ $401$, but we still can limit it using `max_features` parameter, so it is not a big deal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46081855-8f7a-48ea-8ed9-57e022fcab07",
   "metadata": {},
   "source": [
    "##### 3.1.4 Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23cf3fb-1182-4d94-ba3a-560762803d9c",
   "metadata": {},
   "source": [
    "Removing or not removing stopwords is a controversial topic..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1766375f-9b1e-4f6d-9bd4-d96f33e295c6",
   "metadata": {},
   "source": [
    "##### 3.1.5 Advantages and disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b22f247-c293-4bad-a549-7153c2afd529",
   "metadata": {},
   "source": [
    "$\"+\"$:\n",
    "* Fast to train (actually, there is no training done - just counting)\n",
    "\n",
    "$\"-\"$:\n",
    "* Vector dimensionality is equal to vocabulary size (`max_features`) \n",
    "* Longer documents will have higher average count values than shorter documents, even though they might talk about the same topics\n",
    "* **One-hot vectors don't capture meaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf817e-aa93-4980-8cf3-6fd9e087f51a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "760c80bf-6cc5-47e8-b0f0-264877fa5843",
   "metadata": {},
   "source": [
    "### TODO: \n",
    "* Finish item 3.1.4 about using stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2164732d-a380-4489-ac15-6614d3b45de8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
