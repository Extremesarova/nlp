{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2027fc63-e425-4c3a-b45a-e89e567eae6e",
   "metadata": {},
   "source": [
    "### 1. Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2cacc5-f7f3-4082-9979-6679e60a39e5",
   "metadata": {},
   "source": [
    "First, let's import needed modules and, random seed (we'll use it if needed) and create some auxiliary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "581b5387-0b6f-4d93-860e-ef0aba2c7579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import islice, chain\n",
    "from nltk import ngrams\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Tuple, Iterator\n",
    "\n",
    "SEED=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31d0be56-84d2-4b3d-9123-0b0b41ff10e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52826a9c-fd97-48f3-ada4-104915ec76da",
   "metadata": {},
   "source": [
    "### 2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dff2098-0287-41b6-ad17-465a98f9ac96",
   "metadata": {},
   "source": [
    "I'll be using dataset from [Spooky Author Identification](https://www.kaggle.com/c/spooky-author-identification/overview) competition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256b5b8b-0ad8-4e21-8f70-ebc9bd317cb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1 Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ffa751c-18dc-4ce4-a34b-32963e704960",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a823ea3-0b17-4fb4-a81b-f79e36e88a59",
   "metadata": {},
   "source": [
    "#### 2.2 Data Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cb1cb5-9541-4db5-ab53-d2eb4a422da8",
   "metadata": {},
   "source": [
    "* id - a unique identifier for each sentence\n",
    "* text - some text written by one of the authors\n",
    "* author - the author of the sentence (EAP: Edgar Allan Poe, HPL: HP Lovecraft; MWS: Mary Wollstonecraft Shelley)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4849f6-e2d4-4ee1-9098-755d7b5f3c74",
   "metadata": {},
   "source": [
    "Let's look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7855d2ae-7f71-4cc1-b5b1-bbe01b63d113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7acaa22-858e-4962-a3d9-361228e0c635",
   "metadata": {},
   "source": [
    "For now, I'm going to look only at column text to look at the ways text representation can be done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff621cc0-8965-4b9d-863a-213b997ff0be",
   "metadata": {},
   "source": [
    "#### 2.3 Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed7ab0b-4174-4728-969c-ceb3635c23c5",
   "metadata": {},
   "source": [
    "But nevertheless let's split the data into training and validation sets.  \n",
    "As soon as we have almost $20 000$ rows in `train_df`, test size will be limited to $10\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0154461-4b3a-445e-b439-2930233bc107",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(train_df, test_size=0.1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2ac83c-4476-4a0c-abb8-047e51f51947",
   "metadata": {},
   "source": [
    "### 3. Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20bed68-aa67-4d14-a45a-1b0956409fbd",
   "metadata": {},
   "source": [
    "#### 3.1 Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a612f62b-d324-4892-904d-72d3a3e719ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 3.1.1 One-hot vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632d2a19-43a6-42c3-b6d6-b2ec2b48789f",
   "metadata": {},
   "source": [
    "The simplest way of word representation is **one-hot vectors**. For the i-th word in the vocabulary, the vector has 1 on the i-th dimension and 0 on the rest.   \n",
    "Let's do this using sklearn's `CountVectorizer` with `binary=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b186ea0d-8a8a-4aba-bed2-4dc5059e9699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (17621, 24069)\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(binary=True)\n",
    "X_train_oh = count_vect.fit_transform(train['text'])\n",
    "print(f\"The size of the train dataset is {X_train_oh.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a67895-fc80-4251-b5e5-bd1c3794a88e",
   "metadata": {},
   "source": [
    "By default, we are not limiting the vocabulary of the model and the length of the vector for every sentence will be $24$ $069$ - number of words in our vocab.\n",
    "Although, it can be done by setting parameter `max_features` to, for example, $10000$. By doing this, vocabulary will be built considering only the top `max_features` ordered by term frequency across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b66834b-b43c-41ed-bc27-ad31ef1b29d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<17621x24069 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 386417 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_oh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4346339-6c2b-4a7f-b656-29c62ad0b418",
   "metadata": {},
   "source": [
    "It is also worth to mention, that due to the sparsity of representation (most values in word vectors will be zeros) we can save a lot of memory by only storing the non-zero parts of the feature vectors in memory. `scipy.sparse` matrices are data structures that do exactly this and they are used in `sklearn` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0835806a-fd90-432c-b114-18da4b67c901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "424119849"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "17621*24069"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef0dbef-8b0c-4dad-a68b-84e9ab2c2ef3",
   "metadata": {},
   "source": [
    "In our case only $386$ $417$ of elements out of $424$ $119$ $849$ are non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33cc670b-58d1-42ed-9c8c-dde458d946d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17621, 10000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect_lim_vocab = CountVectorizer(binary=True, max_features=10_000)\n",
    "X_train_oh = count_vect_lim_vocab.fit_transform(train['text'])\n",
    "X_train_oh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0aa8a-cbf5-4065-8149-2ec629598d4c",
   "metadata": {},
   "source": [
    "Now, the length of the vector is $10000$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b62305-0e58-4e9e-b33b-bbb0ef92518b",
   "metadata": {},
   "source": [
    "Let's look at the vector for the first text in our train corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df405b94-371d-46b9-bfa1-d6c68b251174",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence = val['text'][6148]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e24d901b-d0ae-4c31-90a3-1b43e9886f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x24069 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 16 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_vector = count_vect.transform([first_sentence])\n",
    "one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37edd75b-1795-46f9-b6b6-a221afb16fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_vector = one_hot_vector.toarray()\n",
    "one_hot_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6b4454-cba2-4e07-86ad-4ee95a66a4a3",
   "metadata": {},
   "source": [
    "It is a sparse vector with $24$ $069$ elements with only $16$ elements that are not equal to zero. \n",
    "Let's find out which are they."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "163f00ab-8a9a-453a-a896-5595ebf5cbf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  614,   813,  1548,  2128,  5189,  9205, 10643, 12834, 14023,\n",
       "       14557, 20543, 21197, 21300, 21556, 22713, 23613], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.where(np.any(one_hot_vector!=0, axis=0))[0]\n",
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711b8e60-d43a-477c-9a16-178c3e62d33f",
   "metadata": {},
   "source": [
    "These are the indices of words which are present in our sentence.  \n",
    "Now we are going to create index-to-word dictionary to check the result of the work of `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "585e32cb-1ddb-4ef2-b314-2d276461f389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{20160: 'still',\n",
       " 14787: 'others',\n",
       " 10869: 'including',\n",
       " 11721: 'joe',\n",
       " 10157: 'himself',\n",
       " 9894: 'have',\n",
       " 21222: 'theories',\n",
       " 21548: 'too',\n",
       " 23652: 'wild',\n",
       " 813: 'and'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word = {index : word for word, index in count_vect.vocabulary_.items()}\n",
    "dict(take(10, index_to_word.items()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1477509-68cf-4784-aeac-b793232d51a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(614, 'all', 1),\n",
       " (813, 'and', 1),\n",
       " (1548, 'available', 1),\n",
       " (2128, 'bewildered', 1),\n",
       " (5189, 'dazzled', 1),\n",
       " (9205, 'gigantic', 1),\n",
       " (10643, 'immediately', 1),\n",
       " (12834, 'magnitude', 1),\n",
       " (14023, 'nature', 1),\n",
       " (14557, 'of', 1),\n",
       " (20543, 'sum', 1),\n",
       " (21197, 'the', 1),\n",
       " (21300, 'thought', 1),\n",
       " (21556, 'topic', 1),\n",
       " (22713, 'upon', 1),\n",
       " (23613, 'who', 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(ind, index_to_word[ind], one_hot_vector[0, ind]) for ind in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc193b3-57cf-4829-9424-a0841befb951",
   "metadata": {},
   "source": [
    "Indeed, these are the indices and corresponding words from our sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26827458-5e5f-48cb-99c1-fc2fae032fde",
   "metadata": {},
   "source": [
    "Because we've created the `CountVectorizer` with `binary=True`. The elements are really ones and zeros.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d0a03-a27f-4def-882b-59ca316df9da",
   "metadata": {},
   "source": [
    "##### 3.1.2 One-hot vectors with counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6b28ef-3ba7-40cd-99e4-52d594953c28",
   "metadata": {},
   "source": [
    "A little improvement over that will be using vectorizer with `binary=False`, because this way we will take counts into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c6158ae-5dc7-4ee0-84d1-f91d3bb5f6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(614, 'all', 1),\n",
       " (813, 'and', 2),\n",
       " (1548, 'available', 1),\n",
       " (2128, 'bewildered', 1),\n",
       " (5189, 'dazzled', 1),\n",
       " (9205, 'gigantic', 1),\n",
       " (10643, 'immediately', 1),\n",
       " (12834, 'magnitude', 1),\n",
       " (14023, 'nature', 1),\n",
       " (14557, 'of', 1),\n",
       " (20543, 'sum', 1),\n",
       " (21197, 'the', 4),\n",
       " (21300, 'thought', 1),\n",
       " (21556, 'topic', 1),\n",
       " (22713, 'upon', 1),\n",
       " (23613, 'who', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(binary=False)\n",
    "X_train_cv = count_vect.fit_transform(train['text'])\n",
    "\n",
    "one_hot_vector = count_vect.transform([first_sentence]).toarray()\n",
    "indices = np.where(np.any(one_hot_vector!=0, axis=0))[0]\n",
    "index_to_word = {index : word for word, index in count_vect.vocabulary_.items()}\n",
    "\n",
    "[(ind, index_to_word[ind], one_hot_vector[0, ind]) for ind in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8553e30c-b37e-44a4-a8ca-b7a2c71ea00a",
   "metadata": {},
   "source": [
    "We can see that now the value for 'the' is 4 and for 'two' it is 2. It doesn't only show that these words are present in the sentence, but also indicate how many times they occur in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1201b65-a554-45e3-90bf-95b2e4c5f124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = count_vect.build_tokenizer()\n",
    "tokenized_sentence = tokenizer(first_sentence.lower())\n",
    "tokenized_sentence.count('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c827759a-7dc9-4358-9617-31635a650874",
   "metadata": {},
   "source": [
    "##### 3.1.3 N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00ab815-bb23-42a7-a957-26ef2e5081db",
   "metadata": {},
   "source": [
    "We can take into account not only words, but collocations using parameter `ngram_range` to preserve some local ordering, because by using unigrams we don't capture even that.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57503039-5cf5-4a7f-919e-2a9ccf11b8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (17621, 230440)\n",
      "<bound method _data_matrix.count_nonzero of <17621x230440 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 813907 stored elements in Compressed Sparse Row format>>\n",
      "[(4324, 'all', 1), (4855, 'all who', 1), (7667, 'and', 2), (8116, 'and bewildered', 1), (12202, 'and the', 1), (19515, 'available', 1), (25467, 'bewildered', 1), (45745, 'dazzled', 1), (75382, 'gigantic', 1), (93018, 'immediately', 1), (112540, 'magnitude', 1), (112541, 'magnitude and', 1), (124876, 'nature', 1), (124942, 'nature of', 1), (131308, 'of', 1), (134836, 'of the', 1), (181040, 'sum', 1), (186670, 'the', 4), (189413, 'the gigantic', 1), (192991, 'the sum', 1), (193291, 'the topic', 1), (199054, 'thought', 1), (199173, 'thought upon', 1), (203935, 'topic', 1), (209892, 'upon', 1), (210124, 'upon the', 1), (222194, 'who', 1), (222471, 'who thought', 1)]\n"
     ]
    }
   ],
   "source": [
    "bigram_count_vect = CountVectorizer(binary=False, ngram_range=(1,2))\n",
    "X_train_bigram = bigram_count_vect.fit_transform(train['text'])\n",
    "print(f\"The size of the train dataset is {X_train_bigram.shape}\")\n",
    "print(X_train_bigram.count_nonzero)\n",
    "one_hot_vector = bigram_count_vect.transform([first_sentence]).toarray()\n",
    "indices = np.where(np.any(one_hot_vector!=0, axis=0))[0]\n",
    "index_to_word = {index : word for word, index in bigram_count_vect.vocabulary_.items()}\n",
    "\n",
    "print([(ind, index_to_word[ind], one_hot_vector[0, ind]) for ind in indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c8e685-d4dc-4fd3-997a-922dfe4db161",
   "metadata": {},
   "source": [
    "By including bigrams into calculations the size of the vector increased from $24$ $069$ to $230$ $440$, but we still can limit it using `max_features` parameter, so it is not a big deal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46081855-8f7a-48ea-8ed9-57e022fcab07",
   "metadata": {},
   "source": [
    "##### 3.1.4 Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23cf3fb-1182-4d94-ba3a-560762803d9c",
   "metadata": {},
   "source": [
    "Removing or not removing stopwords is a controversial topic..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1766375f-9b1e-4f6d-9bd4-d96f33e295c6",
   "metadata": {},
   "source": [
    "##### 3.1.5 Advantages and disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b22f247-c293-4bad-a549-7153c2afd529",
   "metadata": {},
   "source": [
    "$\"+\"$:\n",
    "1. Fast to train (actually, there is no training done - just counting)\n",
    "\n",
    "$\"-\"$:\n",
    "1. Vector dimensionality is equal to vocabulary size (`max_features`) \n",
    "2. Longer documents will have higher average count values than shorter documents, even though they might talk about the same topics\n",
    "3. **One-hot vectors don't capture meaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa353f98-f48b-4900-8892-72fb8c5e2fd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 3.1.6 Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aca942-5d77-4cc7-aeba-c5011fb7e884",
   "metadata": {},
   "source": [
    "To evaluate embeddings we will stick to extrinsic evaluation - evaluation on real task - text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc098168-8af9-4c3b-a422-6dbdf042ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_eval = CountVectorizer(ngram_range=(1,1))\n",
    "X_train = count_vect_eval.fit_transform(train['text'])\n",
    "X_val = count_vect_eval.transform(val['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03a066e7-4afe-421b-8e39-95b151d4c8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000, solver='sag')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_count = LogisticRegression(solver='sag', max_iter=10000)\n",
    "logreg_count.fit(X_train, train['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "620288f7-2a5e-4f3a-8f9f-80d1a6da4abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8226872991519389"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = logreg_count.predict(X_val)\n",
    "f1 = f1_score(pred, val['author'], average='macro')\n",
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf817e-aa93-4980-8cf3-6fd9e087f51a",
   "metadata": {},
   "source": [
    "#### 3.2 TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09125b26-3db8-40bd-9072-684d5e931592",
   "metadata": {},
   "source": [
    "##### 3.2.1 Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a597f7c-fc26-4512-982d-49fd02f5c10f",
   "metadata": {},
   "source": [
    "To address second issue of bag of words approach and in attempt of trying to deal with common words (stopwords) without deleting them the next approach can be used.  \n",
    "\n",
    "Tf means term-frequency while tf–idf means term-frequency times inverse document-frequency:\n",
    "$$ \\large tf-idf(t,d) = tf(i,d) * idf(t)$$  \n",
    "Term frequency is: \n",
    "$$ \\large tf(i,d) = \\frac{wordCount(t,d)}{length(d)}$$ \n",
    "where:\n",
    "* `wordCount(t,d)` is the number of occurrences of term *t* in the document *d*\n",
    "* `length(d)` is the number of words in the document *d*  \n",
    "\n",
    "Dividing the number of occurrences of each word in a document by the total number of words in the document helps to solve the issue mentioned in the first sentence of this block.\n",
    "\n",
    "Inverse document-frequency is:\n",
    "$$ \\large idf(t, c) = \\frac{size(c)}{docCount(t, c)}$$ \n",
    "where: \n",
    "* `size(c)` is the number of documents in the corpora *c*\n",
    "* `docCount(t, c)` is the number of documents in corpora *c* containing the term *t*  \n",
    "\n",
    "Ifd helps to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a73ca9-c3cf-44aa-9b24-e49db72a06a8",
   "metadata": {},
   "source": [
    "##### 3.2.2 Real life form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefcba7d-0cbd-427e-b5ed-b223b804ea45",
   "metadata": {},
   "source": [
    "In real life tf-idf is computed differently:\n",
    "$$ \\large tf(i,d) = \\log{ \\left(1 + \\frac{wordCount(t,d)}{length(d)}\\right)}$$ \n",
    "with `sublinear_tf=True` for `sklearn.feature_extraction.text.TfidfVectorizer`\n",
    "$$ \\large idf(t, c) = 1 + \\log{\\left(\\frac{1 + size(c)}{1 + docCount(t, c)}\\right)}$$ \n",
    "with `norm='l2', use_idf=True, smooth_idf=True` for `sklearn.feature_extraction.text.TfidfVectorizer`\n",
    "\n",
    "The reason to use `log` here is this. \n",
    "\n",
    "After that the vector with tf-idf scores for the words in the sentence should be normalized by the Euclidean norm:\n",
    "$$\\large v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v_{1}^2 + v_{2}^2 + ... + v_{n}^2}}$$\n",
    "By doing this each output vector will have l2 unit norm. The cosine similarity between two vectors is their dot product when l2 norm has been applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691ee8a1-312c-4f7f-a1d6-5f1eab689a25",
   "metadata": {},
   "source": [
    "##### 3.2.3 Sklearn's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "256b7a89-4979-4469-8e9f-a28cd462282d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (17621, 24069)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer()\n",
    "X_train_tf_idf = tfidf_vect.fit_transform(train['text'])\n",
    "print(f\"The size of the train dataset is {X_train_tf_idf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81173fba-5399-40fb-86fe-6dde8ae9cd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<17621x24069 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 386417 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da35a131-a399-4643-89a8-45d8d1a4260b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x24069 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect.transform([first_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b20f545b-cbae-4a95-a044-37269b46dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index : word for word, index in tfidf_vect.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3223bca9-daa5-4af9-8c3b-20818ca9714f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x24069 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vector = tfidf_vect.transform([first_sentence])\n",
    "tfidf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7861bfdd-4fb1-4e86-b3e1-83ba06663f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vector = tfidf_vector.toarray()\n",
    "tfidf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a6e09e7-e84e-4db9-856f-304c789a4175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  614,   813,  1548,  2128,  5189,  9205, 10643, 12834, 14023,\n",
       "       14557, 20543, 21197, 21300, 21556, 22713, 23613], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.where(np.any(tfidf_vector!=0, axis=0))[0]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13944345-6a7a-43c3-afa7-a135ed1d6512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(614, 'all', 0.13031453897504672),\n",
       " (813, 'and', 0.12214845066990851),\n",
       " (1548, 'available', 0.3924242175008267),\n",
       " (2128, 'bewildered', 0.3114996887796031),\n",
       " (5189, 'dazzled', 0.36544937459375215),\n",
       " (9205, 'gigantic', 0.2695648987205094),\n",
       " (10643, 'immediately', 0.23647294743920957),\n",
       " (12834, 'magnitude', 0.32608143594739347),\n",
       " (14023, 'nature', 0.2049931029681656),\n",
       " (14557, 'of', 0.059811003957834306),\n",
       " (20543, 'sum', 0.3009169879304377),\n",
       " (21197, 'the', 0.20393197647953104),\n",
       " (21300, 'thought', 0.18711801923161203),\n",
       " (21556, 'topic', 0.2973766872628445),\n",
       " (22713, 'upon', 0.14506693111726507),\n",
       " (23613, 'who', 0.16239686486865726)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(ind, index_to_word[ind], tfidf_vector[0, ind]) for ind in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc89f53c-4a90-44b5-a697-a32a468bdb05",
   "metadata": {},
   "source": [
    "We see that the words that are popular among the sentences in the corpus have a much lower tf-idf score, than less popular ones.\n",
    "For example, words that have:\n",
    "* lowest scores: all, an, of, upon *occur often in the sentences, thus cannot help to discern the sentences between themselves*.\n",
    "* highest scores: available, dazzled, magnitude, sum, topic *are probably rarely found in the sentences in the corpus of texts, but are available in this sentence and thus will make a good features*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4147fad0-2da3-4fb6-a1e8-7bb3352d2020",
   "metadata": {},
   "source": [
    "##### 3.2.4 TF-IDF implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9918f89e-c9af-4ab5-b23c-1e53e01c1d7a",
   "metadata": {},
   "source": [
    "I'm going to implement tf-idf from scratch and compare results with sklearn's implementation.  \n",
    "However, to save time I will use already trained tokenizer from sklearn.  \n",
    "*Disclaimer: this implementation is not optimized and not tested properly*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35ab9fa5-fbf6-498a-9ef0-c8b37ac82070",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_tokenizer = tfidf_vect.build_tokenizer()\n",
    "first_sentence_tokenized = tfidf_tokenizer(first_sentence.lower())\n",
    "first_sentence_tokenized_set = list(set(first_sentence_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d3d428-027a-47ef-90f4-e779344bb77d",
   "metadata": {},
   "source": [
    "I will get the scores for the same sentence 'The gigantic magnitude and the immediately available nature of the sum, dazzled and bewildered all who thought upon the topic.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c3dc334-7b3e-44b6-9446-cb191a9da3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf(tokenized_text, term):\n",
    "    return tokenized_text.count(term) / len(tokenized_text)\n",
    "\n",
    "def get_idf(tokenized_corpus, term, tokenizer):\n",
    "    size = len(tokenized_corpus)\n",
    "    doc_count = sum([1 if term in text else 0 for text in tokenized_corpus])\n",
    "    \n",
    "    return np.log((1 + size) / (1 + doc_count)) + 1\n",
    "\n",
    "def get_tf_idf(tf, idf):\n",
    "    return tf * idf\n",
    "\n",
    "def get_l2_norm(vector):\n",
    "    return np.sqrt(np.sum(vector**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd26cc47-e247-4cb2-96dd-3a6a28dcba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_first_sentece = []\n",
    "tokenized_corpus = [tokenizer(text.lower()) for text in train['text']]\n",
    "\n",
    "for word in first_sentence_tokenized_set:\n",
    "    tf_ = get_tf(first_sentence_tokenized, word)\n",
    "    idf_ = get_idf(tokenized_corpus, word, tfidf_tokenizer)\n",
    "    tf_idf_first_sentece.append(get_tf_idf(tf_, idf_))\n",
    "    \n",
    "tf_idf_first_sentece = np.array(tf_idf_first_sentece)\n",
    "l2_norm = get_l2_norm(tf_idf_first_sentece)\n",
    "tf_idf_first_sentece = tf_idf_first_sentece / l2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c22c01c-27fe-48fb-a803-bdbdb77cf832",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_first_sentence_dict = dict(zip(first_sentence_tokenized_set, tf_idf_first_sentece))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1651ecaa-68b1-4e42-b33b-cf23e5c7dd3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(614, 'all', 0.13031453897504672),\n",
       " (813, 'and', 0.12214845066990852),\n",
       " (1548, 'available', 0.3924242175008267),\n",
       " (2128, 'bewildered', 0.3114996887796031),\n",
       " (5189, 'dazzled', 0.36544937459375215),\n",
       " (9205, 'gigantic', 0.2695648987205094),\n",
       " (10643, 'immediately', 0.23647294743920957),\n",
       " (12834, 'magnitude', 0.32608143594739347),\n",
       " (14023, 'nature', 0.2049931029681656),\n",
       " (14557, 'of', 0.05981100395783431),\n",
       " (20543, 'sum', 0.3009169879304377),\n",
       " (21197, 'the', 0.20393197647953104),\n",
       " (21300, 'thought', 0.18711801923161206),\n",
       " (21556, 'topic', 0.2973766872628445),\n",
       " (22713, 'upon', 0.1450669311172651),\n",
       " (23613, 'who', 0.16239686486865726)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(ind, index_to_word[ind], tf_idf_first_sentence_dict[index_to_word[ind]]) for ind in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a45a9a9-2439-4c17-9deb-a79a90cf7637",
   "metadata": {},
   "source": [
    "In the end, we are getting the same results as in sklearn's implementation with default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583468e9-26cc-4817-8b74-ef1eba84179a",
   "metadata": {},
   "source": [
    "##### 3.2.5 Main parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aaa45c-9f6f-46a1-b96b-ae317c414925",
   "metadata": {},
   "source": [
    "* `strip_accents`: Remove accents and perform other character normalization during the preprocessing step. \n",
    "* `stop_words`: If a string, it is passed to _check_stop_list and the appropriate stop list is returned. ‘english’ is currently the only supported string value.\n",
    "* `ngram_range`: The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams.\n",
    "* `max_df`: When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float in range [0.0, 1.0], the parameter represents a proportion of documents, integer absolute counts.\n",
    "* `min_df`: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float in range of [0.0, 1.0], the parameter represents a proportion of documents, integer absolute counts.\n",
    "* `max_features`: If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "* `binary`: If True, all non-zero term counts are set to 1. This does not mean outputs will have only 0/1 values, only that the tf term in tf-idf is binary. (Set idf and normalization to False to get 0/1 outputs).\n",
    "* `norm`: Each output row will have unit norm, either:\n",
    "    * ‘l2’: Sum of squares of vector elements is 1. The cosine similarity between two vectors is their dot product when l2 norm has been applied.\n",
    "    * ‘l1’: Sum of absolute values of vector elements is 1. See preprocessing.normalize.\n",
    "* `use_idf`: Enable inverse-document-frequency reweighting. If False, idf(t) = 1.\n",
    "* `smooth_idf`: Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions.\n",
    "* `sublinear_tf`: Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310356e0-0b0f-4a2b-855e-e3a3e8b2deb1",
   "metadata": {},
   "source": [
    "Hyperparameters to use for optimization:\n",
    "* max_df\n",
    "* min_df\n",
    "* max_features\n",
    "* ngram_range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9267e542-f712-403f-aea6-3b1eae04fb69",
   "metadata": {},
   "source": [
    "##### 3.2.6 N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7e4d082-ecbb-4f47-90da-ee6850bea117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is (17621, 230440)\n",
      "<bound method _data_matrix.count_nonzero of <17621x230440 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 813907 stored elements in Compressed Sparse Row format>>\n",
      "The gigantic magnitude and the immediately available nature of the sum, dazzled and bewildered all who thought upon the topic.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(4324, 'all', 0.088547269675905),\n",
       " (4855, 'all who', 0.2335206648733459),\n",
       " (7667, 'and', 0.0829985041349335),\n",
       " (8116, 'and bewildered', 0.24831875690585964),\n",
       " (12202, 'and the', 0.09948207535251821),\n",
       " (19515, 'available', 0.26664786053577216),\n",
       " (25467, 'bewildered', 0.21166054964603465),\n",
       " (45745, 'dazzled', 0.24831875690585964),\n",
       " (75382, 'gigantic', 0.18316632948172856),\n",
       " (93018, 'immediately', 0.16068071922477745),\n",
       " (112540, 'magnitude', 0.22156868352708564),\n",
       " (112541, 'magnitude and', 0.24831875690585964),\n",
       " (124876, 'nature', 0.13929051748936838),\n",
       " (124942, 'nature of', 0.1894923535718752),\n",
       " (131308, 'of', 0.040640907290130535),\n",
       " (134836, 'of the', 0.06654221632472784),\n",
       " (181040, 'sum', 0.20446972294810256),\n",
       " (186670, 'the', 0.13856949392524126),\n",
       " (189413, 'the gigantic', 0.24831875690585964),\n",
       " (192991, 'the sum', 0.2335206648733459),\n",
       " (193291, 'the topic', 0.23759691861052884),\n",
       " (199054, 'thought', 0.12714459829609193),\n",
       " (199173, 'thought upon', 0.2559260222404413),\n",
       " (203935, 'topic', 0.20206412829678597),\n",
       " (209892, 'upon', 0.09857135490581051),\n",
       " (210124, 'upon the', 0.12493971282019907),\n",
       " (222194, 'who', 0.11034685078999522),\n",
       " (222471, 'who thought', 0.23759691861052884)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tfidf_vect = TfidfVectorizer(ngram_range=(1,2))\n",
    "\n",
    "X_train_bigram_tfidf = bigram_tfidf_vect.fit_transform(train['text'])\n",
    "\n",
    "bigram_tfidf_vector = bigram_tfidf_vect.transform([first_sentence]).toarray()\n",
    "indices = np.where(np.any(bigram_tfidf_vector!=0, axis=0))[0]\n",
    "index_to_word = {index : word for word, index in bigram_tfidf_vect.vocabulary_.items()}\n",
    "\n",
    "print(f\"The size of the train dataset is {X_train_bigram_tfidf.shape}\")\n",
    "print(X_train_bigram_tfidf.count_nonzero)\n",
    "print(first_sentence)\n",
    "\n",
    "[(ind, index_to_word[ind], bigram_tfidf_vector[0, ind]) for ind in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c0ce4b-2574-41d6-8cb6-f7c146fd368c",
   "metadata": {},
   "source": [
    "Using bigrams (trigrams) will probably greatly improve the performance of the solution based on tf-idf embeddings.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be3208a-ad62-4bcb-bdcc-bba9f3587940",
   "metadata": {},
   "source": [
    "##### 3.2.7 Advantages and disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9e59a0-1b3a-42ba-a7c2-f3d42984fc62",
   "metadata": {},
   "source": [
    "$\"+\"$:\n",
    "1. Outperforms 'bag of words' approach and solves some of its problems\n",
    "\n",
    "$\"-\"$:\n",
    "1. Still doesn't capture meaning\n",
    "\n",
    "Generally, Tf-Idf can be used as a baseline technique for producing text embeddings and text classification (for example, tf-idf + LogReg with hyperparameter optimization).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b78e37-4812-4331-8596-159a3bf1fb1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 3.2.8 Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4cb176-b9df-458c-8fc5-0205626c6c13",
   "metadata": {},
   "source": [
    "To evaluate embeddings we will stick to extrinsic evaluation - evaluation on real task - text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd2da3d2-661f-4aa4-957c-5dc02407e1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect_eval = TfidfVectorizer(ngram_range=(1,1))\n",
    "X_train_tfidf = tfidf_vect_eval.fit_transform(train['text'])\n",
    "X_val_tfidf = tfidf_vect_eval.transform(val['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d6eb283-7195-4d82-a464-bbf22d52aecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000, solver='sag')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_tfidf = LogisticRegression(solver='sag', max_iter=10000)\n",
    "logreg_tfidf.fit(X_train_tfidf, train['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0797b962-0471-48cb-b2c3-70a37f838dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.816697787096243"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = logreg_tfidf.predict(X_val_tfidf)\n",
    "f1 = f1_score(pred, val['author'], average='macro')\n",
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafaad38-2585-48ab-9b0f-38b277cc1e75",
   "metadata": {},
   "source": [
    "#### 3.3 Distributional Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8ba0d9-d1a5-4474-ae6a-2c85aa77657d",
   "metadata": {},
   "source": [
    "Words which frequently appear in similar contexts have similar meaning.\n",
    "\n",
    "Often you can find it formulated as \"You shall know a word by the company it keeps\" with the reference to J. R. Firth in 1957, but actually there were a lot more people responsible, and much earlier. For example, Harris, 1954.\n",
    "\n",
    "This is an extremely valuable idea: it can be used in practice to make word vectors capture their meaning. According to the distributional hypothesis, \"to capture meaning\" and \"to capture contexts\" are inherently the same. Therefore, all we need to do is to put information about word contexts into word representation.\n",
    "\n",
    "Main idea: We need to put information about word contexts into word representation.\n",
    "\n",
    "[Distributional Semantics from Lena Voita's NLP Course | For You](https://lena-voita.github.io/nlp_course/word_embeddings.html#distributional_semantics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8155d67e-4731-4586-bfbf-d86488b60caf",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.4 Count-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304419c0-cff9-40fc-bff0-0bc6fc8826ca",
   "metadata": {},
   "source": [
    "Text about count-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382c74c9-b95c-4f8c-b62b-c15a86bcedb7",
   "metadata": {},
   "source": [
    "##### 3.4.1 Co-Occurence Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ef5705-56b7-43e2-aa6d-a94cae13084f",
   "metadata": {},
   "source": [
    "Text about Co-Occurence Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f053ea0d-2240-400a-8fb8-9e6dbbd020bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = [tokenizer(text.lower()) for text in train['text']]\n",
    "tokenized_val = [tokenizer(text.lower()) for text in val['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bd84471d-45de-4e9f-912d-4ae458a9d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = sorted(list(set(chain.from_iterable(tokenized_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b88cb544-6250-4836-b885-38ee5271130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccurrence_dataframe = pd.DataFrame(0, columns=unique_words, index=unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8ead2d2a-1313-4c04-995f-dfb91666f799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['still',\n",
       " 'others',\n",
       " 'including',\n",
       " 'joe',\n",
       " 'himself',\n",
       " 'have',\n",
       " 'theories',\n",
       " 'too',\n",
       " 'wild',\n",
       " 'and',\n",
       " 'fantastic',\n",
       " 'for',\n",
       " 'sober',\n",
       " 'credence']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d6c77edf-79a8-4748-9197-39f471435a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cooccurrence_dataframe.loc['still', 'others']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5d579a2a-16a9-4c7e-aaff-0c1116cf89c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window(text: List[str], window_size: int) -> Iterator[Tuple[str, List[str]]]:\n",
    "    for backward, current in enumerate(range(len(text)), start=0 - (window_size // 2)):\n",
    "        if backward < 0:\n",
    "            backward = 0\n",
    "        context = list(text[backward:current]) + list(text[current + 1:current + 1 + window_size // 2])\n",
    "        center = text[current]\n",
    "        yield center, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f6bc60b8-5916-4655-9c61-ffac6cafe951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "still ['others', 'including']\n",
      "others ['still', 'including', 'joe']\n",
      "including ['still', 'others', 'joe', 'himself']\n",
      "joe ['others', 'including', 'himself', 'have']\n",
      "himself ['including', 'joe', 'have', 'theories']\n",
      "have ['joe', 'himself', 'theories', 'too']\n",
      "theories ['himself', 'have', 'too', 'wild']\n",
      "too ['have', 'theories', 'wild', 'and']\n",
      "wild ['theories', 'too', 'and', 'fantastic']\n",
      "and ['too', 'wild', 'fantastic', 'for']\n",
      "fantastic ['wild', 'and', 'for', 'sober']\n",
      "for ['and', 'fantastic', 'sober', 'credence']\n",
      "sober ['fantastic', 'for', 'credence']\n",
      "credence ['for', 'sober']\n"
     ]
    }
   ],
   "source": [
    "for center, context_values in get_window(tokenized_train[0], 5):\n",
    "    print(center, context_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c130b10-1143-4ab9-a768-6c25e2c9b81f",
   "metadata": {},
   "source": [
    "step 1: construct a word-context matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5a757d97-b380-48f9-b572-6d43f6041bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc64f1ca50f642adbf06201c8adf4a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "window_size = 5\n",
    "for tokenized_sentence in tqdm(tokenized_train):\n",
    "    for center, context_values in get_window(tokenized_sentence, window_size):\n",
    "        for context_word in context_values:\n",
    "            cooccurrence_dataframe.at[center, context_word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e024c35d-b56f-4339-a1b4-d22c60efc62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cooccurrence_dataframe.loc['still', 'others']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a7d698-41c0-4e8a-a2e8-cb9aa2fddce5",
   "metadata": {},
   "source": [
    "step 2: reduce its dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8fe05a31-53e0-4dd5-983e-a7c54bdb63f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(n_components=200, n_iter=7, random_state=42)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = csr_matrix(cooccurrence_dataframe)\n",
    "svd = TruncatedSVD(n_components=200, n_iter=7, random_state=SEED)\n",
    "svd.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "07696ed2-b457-47e5-a540-642787e9ab96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24069, 200)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors = svd.transform(X)\n",
    "word_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b7ffe4-bd4d-45de-9dd8-af75ba1e5124",
   "metadata": {},
   "source": [
    "step 3: normalize word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "12a75b8a-4a36-4f72-8c49-3fb74d600aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "57291f8e-eb66-440e-bbb3-8ce819c39bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00427691, 2.62921217, 1.41296201, ..., 2.23500891, 2.44693   ,\n",
       "       0.99934727])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(word_vectors, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fbc2760e-b444-49af-9d3b-8e1be6c2dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors /= norm(word_vectors, axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "14b7021f-9113-4edb-9b3c-a8f02b3b95ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(word_vectors, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd04936-89dc-422e-8806-054f7cd2f0be",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 3.4.1.1 Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c440ce3f-ad7c-4c0e-a34d-00236001784d",
   "metadata": {},
   "source": [
    "To evaluate embeddings we will stick to extrinsic evaluation - evaluation on real task - text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "576694ce-d7a4-40f1-991b-4bd27c5f4e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_lookup = dict(zip(cooccurrence_dataframe.index.values, word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "029b6fbb-6637-4213-88c5-944723f59d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(tok_sentence, embeddings_lookup):\n",
    "    word_embeddings = []\n",
    "    for word in tok_sentence:\n",
    "        try:\n",
    "            word_embeddings.append(embeddings_lookup[word])\n",
    "        except:\n",
    "            continue\n",
    "    word_embeddings = np.array(word_embeddings)\n",
    "    v = word_embeddings.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(100)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4697f335-f329-45b8-8661-faf7c5fc36bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92c6d45ed4842c9a67294ac224d944e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f30bab62210464e90e9652ca220eec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1958 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_cooc_counts = [sent2vec(tokenized_sentence, embeddings_lookup) for tokenized_sentence in tqdm(tokenized_train)]\n",
    "X_val_cooc_counts = [sent2vec(tokenized_sentence, embeddings_lookup) for tokenized_sentence in tqdm(tokenized_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cb35d37c-8562-468b-a099-48e2d0cb8521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000, solver='sag')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_cooc_counts = LogisticRegression(solver='sag', max_iter=10000)\n",
    "logreg_cooc_counts.fit(X_train_cooc_counts, train['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0c19c7e7-2977-4b08-9918-b10ec677c940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6513907093968357"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = logreg_cooc_counts.predict(X_val_cooc_counts)\n",
    "f1 = f1_score(pred, val['author'], average='macro')\n",
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8892e71-e82e-4a46-820a-ed4402b084b6",
   "metadata": {},
   "source": [
    "intrinsic evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f44c15cb-105f-4a44-99c1-8aef5d60e11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_1 = embeddings_lookup['remain'].reshape(1, -1)\n",
    "embedding_2 = embeddings_lookup['stay'].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bd9bdc76-f484-48a1-8908-5f41cfe9ec3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.82007657]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = cosine_similarity(embedding_1, embedding_2)\n",
    "sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9650e2-8fbf-49dc-a10f-0d415ccaaeb5",
   "metadata": {},
   "source": [
    "##### 3.4.2 Positive Pointwise Mutual Information (PPMI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96127c99-2cc6-4ee7-90ff-0875d70506a8",
   "metadata": {},
   "source": [
    "Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b7b479b6-9c5c-4877-8ab1-d8fa4e5cbe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_dataframe = cooccurrence_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "07b0f68c-4968-45d7-9f70-7c9ab7272e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_cooccurrences = cooccurrence_dataframe.to_numpy().sum()\n",
    "row_counts = cooccurrence_dataframe.sum(axis=0).to_dict()\n",
    "column_counts = cooccurrence_dataframe.sum(axis=1).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "537df315-5519-4e3c-87cb-ab85e8a0a688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0361eb62174d4ebb9bd0fb4ddd5a174b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "window_size = 5\n",
    "for tokenized_sentence in tqdm(tokenized_train):\n",
    "    for center, context_values in get_window(tokenized_sentence, window_size):\n",
    "        for context_word in context_values:\n",
    "            p_i = row_counts[center] / sum_cooccurrences\n",
    "            p_j = column_counts[context_word] / sum_cooccurrences\n",
    "            p_ij = cooccurrence_dataframe.at[center, context_word] / sum_cooccurrences\n",
    "            pmi_dataframe.at[center, context_word] = np.log2(p_ij / (p_i * p_j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fd7463bd-e7c1-400f-bf83-c0f10e48e6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppmi_dataframe = pmi_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "95cb506e-97f5-4349-a85f-273481f469db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppmi_dataframe[:] = np.vectorize(lambda value: max(0, value))(ppmi_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ccb650d9-209e-4a1f-8768-ed1cdd565fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppmi_dataframe.loc['still', 'others']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f1de89-3e7b-4cb6-8bb9-ed6b9f424fac",
   "metadata": {},
   "source": [
    "step 2: reduce its dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c403a901-ff24-44c2-8f24-c915922c91a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(n_components=200, n_iter=7, random_state=42)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = csr_matrix(ppmi_dataframe)\n",
    "svd = TruncatedSVD(n_components=200, n_iter=7, random_state=SEED)\n",
    "svd.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "519e9a7e-f9a0-4135-a342-d859249b74ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24069, 200)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors = svd.transform(X)\n",
    "word_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acde2f31-8fe8-4588-ba5c-c3ab8b30f6cc",
   "metadata": {},
   "source": [
    "step 3: normalize word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a22cb97e-b4ba-48c3-b0e2-a72ce87b1f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7e928185-9881-4c24-bc78-ebf8a1a4e105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.51575276, 7.66269329, 4.17084946, ..., 3.33118166, 4.66171782,\n",
       "       2.8152182 ])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(word_vectors, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "37c5f030-d7e3-456c-96e3-a352d8c64ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors /= norm(word_vectors, axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "90ed1647-412a-43ad-88a1-346f9f3dfe0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(word_vectors, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1add26-f135-4e75-b4ec-45362e737af9",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 3.4.2.1 Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526442cc-6107-4c50-8f18-0a854ac6a3ef",
   "metadata": {},
   "source": [
    "To evaluate embeddings we will stick to extrinsic evaluation - evaluation on real task - text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a98fe85d-442a-4116-86cf-a6cffa4b057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_lookup = dict(zip(ppmi_dataframe.index.values, word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "23caf47b-ff62-4a3c-a73a-18f56df30d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(tok_sentence, embeddings_lookup):\n",
    "    word_embeddings = []\n",
    "    for word in tok_sentence:\n",
    "        try:\n",
    "            word_embeddings.append(embeddings_lookup[word])\n",
    "        except:\n",
    "            continue\n",
    "    word_embeddings = np.array(word_embeddings)\n",
    "    v = word_embeddings.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(100)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e682bc1e-aa6a-41a8-8cb8-9ae4ff283759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be532712287491b8b40809d8edeac29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0383e2b541b4ac3a5d9c343ed065409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1958 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_cooc_ppmi = [sent2vec(tokenized_sentence, embeddings_lookup) for tokenized_sentence in tqdm(tokenized_train)]\n",
    "X_val_cooc_ppmi = [sent2vec(tokenized_sentence, embeddings_lookup) for tokenized_sentence in tqdm(tokenized_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "787f403c-ad20-42c5-8b76-c71eb97b628b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000, solver='sag')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_cooc_counts = LogisticRegression(solver='sag', max_iter=10000)\n",
    "logreg_cooc_counts.fit(X_train_cooc_ppmi, train['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bbd41114-1a7d-4679-a02a-5287a99705b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7644805259796094"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = logreg_cooc_counts.predict(X_val_cooc_ppmi)\n",
    "f1 = f1_score(pred, val['author'], average='macro')\n",
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c80bf-6cc5-47e8-b0f0-264877fa5843",
   "metadata": {},
   "source": [
    "### TODO: \n",
    "* Finish item 3.1.4 about using stopwords \n",
    "* Add the reason to use logarithms for tf-idf (sublinear tf, etc.) [1](https://stats.stackexchange.com/questions/161640/understanding-the-use-of-logarithms-in-the-tf-idf-logarithm) and [2](https://stackoverflow.com/questions/27067992/why-is-log-used-when-calculating-term-frequency-weight-and-idf-inverse-document)\n",
    "* Write down questions\n",
    "    * bag of words\n",
    "    * tf-idf\n",
    "* Fill items 3.3-3.4 with information and refactor the code\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2164732d-a380-4489-ac15-6614d3b45de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.931568569324174"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log2(1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c1b36c69-a189-4dba-8b26-917fc7043687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.931568569324174"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log2(2_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3390b00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5acf0376ddefa89f0f90d79cf9651f12111c46db01e8a73a8bd54c13ffa9735d"
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
