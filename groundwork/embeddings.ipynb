{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "464941a0-c342-4b6b-b3bc-71e7cad3f6eb",
   "metadata": {},
   "source": [
    "### 0. Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb3eac9-69c4-43b2-bf28-09d658012839",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2027fc63-e425-4c3a-b45a-e89e567eae6e",
   "metadata": {},
   "source": [
    "### 1. Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2cacc5-f7f3-4082-9979-6679e60a39e5",
   "metadata": {},
   "source": [
    "First, let's import needed modules and, random seed (we'll use it if needed) and create some auxiliary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "581b5387-0b6f-4d93-860e-ef0aba2c7579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31d0be56-84d2-4b3d-9123-0b0b41ff10e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52826a9c-fd97-48f3-ada4-104915ec76da",
   "metadata": {},
   "source": [
    "### 2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dff2098-0287-41b6-ad17-465a98f9ac96",
   "metadata": {},
   "source": [
    "I'll be using dataset from [Spooky Author Identification](https://www.kaggle.com/c/spooky-author-identification/overview) competition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256b5b8b-0ad8-4e21-8f70-ebc9bd317cb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1 Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ffa751c-18dc-4ce4-a34b-32963e704960",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a823ea3-0b17-4fb4-a81b-f79e36e88a59",
   "metadata": {},
   "source": [
    "#### 2.2 Data Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cb1cb5-9541-4db5-ab53-d2eb4a422da8",
   "metadata": {},
   "source": [
    "* id - a unique identifier for each sentence\n",
    "* text - some text written by one of the authors\n",
    "* author - the author of the sentence (EAP: Edgar Allan Poe, HPL: HP Lovecraft; MWS: Mary Wollstonecraft Shelley)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4849f6-e2d4-4ee1-9098-755d7b5f3c74",
   "metadata": {},
   "source": [
    "Let's look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7855d2ae-7f71-4cc1-b5b1-bbe01b63d113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7acaa22-858e-4962-a3d9-361228e0c635",
   "metadata": {},
   "source": [
    "For now, I'm going to look only at column text to look at the ways text representation can be done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff621cc0-8965-4b9d-863a-213b997ff0be",
   "metadata": {},
   "source": [
    "#### 2.3 Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed7ab0b-4174-4728-969c-ceb3635c23c5",
   "metadata": {},
   "source": [
    "But nevertheless let's split the data into training and validation sets.  \n",
    "As soon as we have almost $20 000$ rows in `train_df` test size will be limited to $10\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0154461-4b3a-445e-b439-2930233bc107",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(train_df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2ac83c-4476-4a0c-abb8-047e51f51947",
   "metadata": {},
   "source": [
    "### 3. Text embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a612f62b-d324-4892-904d-72d3a3e719ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.1 One-hot vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632d2a19-43a6-42c3-b6d6-b2ec2b48789f",
   "metadata": {},
   "source": [
    "The simplest way of word representation is **one-hot vectors**. For the i-th word in the vocabulary, the vector has 1 on the i-th dimension and 0 on the rest.   \n",
    "Let's do this using sklearn's `CountVectorizer` with `binary=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b186ea0d-8a8a-4aba-bed2-4dc5059e9699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17621, 24113)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(binary=True)\n",
    "X_train_cv = count_vect.fit_transform(train['text'])\n",
    "X_train_cv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a67895-fc80-4251-b5e5-bd1c3794a88e",
   "metadata": {},
   "source": [
    "By default, we are not limiting the vocabulary of the model and the length of the vector for every sentence will be $24066$ - number of words in our vocab.\n",
    "Although, it can be done by setting parameter `max_features` to, for example, $10000$. By doing this, vocabulary will be built considering only the top `max_features` ordered by term frequency across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33cc670b-58d1-42ed-9c8c-dde458d946d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17621, 10000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect_lim_vocab = CountVectorizer(binary=True, max_features=10_000)\n",
    "X_train_cv = count_vect_lim_vocab.fit_transform(train['text'])\n",
    "X_train_cv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0aa8a-cbf5-4065-8149-2ec629598d4c",
   "metadata": {},
   "source": [
    "Now, the length of the vector is $10000$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b62305-0e58-4e9e-b33b-bbb0ef92518b",
   "metadata": {},
   "source": [
    "Let's look at the vector for the first text in our train corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df405b94-371d-46b9-bfa1-d6c68b251174",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence = train['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e24d901b-d0ae-4c31-90a3-1b43e9886f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x24113 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 34 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.transform([first_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37edd75b-1795-46f9-b6b6-a221afb16fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_vector = count_vect.transform([first_sentence]).toarray()\n",
    "one_hot_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6b4454-cba2-4e07-86ad-4ee95a66a4a3",
   "metadata": {},
   "source": [
    "It is a sparse vector with $24066$ elements with only $34$ elements that are not equal to zero. \n",
    "Let's find out which are they."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "163f00ab-8a9a-453a-a896-5595ebf5cbf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  441,   808,  1242,  1255,  1597,  1989,  3581,  5892,  6613,\n",
       "        7893, 10358, 11622, 12864, 13138, 13155, 13379, 13925, 14240,\n",
       "       14557, 14809, 15448, 15993, 16469, 17857, 18782, 18923, 19573,\n",
       "       21228, 21314, 21529, 22410, 23320, 23565, 23793], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.where(np.any(one_hot_vector!=0, axis=0))[0]\n",
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711b8e60-d43a-477c-9a16-178c3e62d33f",
   "metadata": {},
   "source": [
    "These are the indices of words which are present in our sentence.  \n",
    "Now we are going to create index-to-word dictionary to check the result of the work of `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "585e32cb-1ddb-4ef2-b314-2d276461f389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{20786: 'swear',\n",
       " 9912: 'he',\n",
       " 4843: 'cried',\n",
       " 2845: 'by',\n",
       " 21228: 'the',\n",
       " 20584: 'sun',\n",
       " 808: 'and',\n",
       " 2323: 'blue',\n",
       " 19389: 'sky',\n",
       " 14557: 'of'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word = {index : word for word, index in count_vect.vocabulary_.items()}\n",
    "dict(take(10, index_to_word.items()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1477509-68cf-4784-aeac-b793232d51a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(441, 'afforded', 1),\n",
       " (808, 'and', 1),\n",
       " (1242, 'as', 1),\n",
       " (1255, 'ascertaining', 1),\n",
       " (1597, 'aware', 1),\n",
       " (1989, 'being', 1),\n",
       " (3581, 'circuit', 1),\n",
       " (5892, 'dimensions', 1),\n",
       " (6613, 'dungeon', 1),\n",
       " (7893, 'fact', 1),\n",
       " (10358, 'however', 1),\n",
       " (11622, 'its', 1),\n",
       " (12864, 'make', 1),\n",
       " (13138, 'me', 1),\n",
       " (13155, 'means', 1),\n",
       " (13379, 'might', 1),\n",
       " (13925, 'my', 1),\n",
       " (14240, 'no', 1),\n",
       " (14557, 'of', 1),\n",
       " (14809, 'out', 1),\n",
       " (15448, 'perfectly', 1),\n",
       " (15993, 'point', 1),\n",
       " (16469, 'process', 1),\n",
       " (17857, 'return', 1),\n",
       " (18782, 'seemed', 1),\n",
       " (18923, 'set', 1),\n",
       " (19573, 'so', 1),\n",
       " (21228, 'the', 1),\n",
       " (21314, 'this', 1),\n",
       " (21529, 'to', 1),\n",
       " (22410, 'uniform', 1),\n",
       " (23320, 'wall', 1),\n",
       " (23565, 'whence', 1),\n",
       " (23793, 'without', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(ind, index_to_word[ind], one_hot_vector[0, ind]) for ind in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc193b3-57cf-4829-9424-a0841befb951",
   "metadata": {},
   "source": [
    "Indeed, these are the indices and corresponding words from our sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26827458-5e5f-48cb-99c1-fc2fae032fde",
   "metadata": {},
   "source": [
    "Because we've created the `CountVectorizer` with `binary=True`. The elements are really ones and zeros.  \n",
    "A little improvement over that will be using vectorizer with `binary=False`, because this way we will take counts into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c6158ae-5dc7-4ee0-84d1-f91d3bb5f6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(441, 'afforded', 1),\n",
       " (808, 'and', 1),\n",
       " (1242, 'as', 1),\n",
       " (1255, 'ascertaining', 1),\n",
       " (1597, 'aware', 1),\n",
       " (1989, 'being', 1),\n",
       " (3581, 'circuit', 1),\n",
       " (5892, 'dimensions', 1),\n",
       " (6613, 'dungeon', 1),\n",
       " (7893, 'fact', 1),\n",
       " (10358, 'however', 1),\n",
       " (11622, 'its', 1),\n",
       " (12864, 'make', 1),\n",
       " (13138, 'me', 1),\n",
       " (13155, 'means', 1),\n",
       " (13379, 'might', 1),\n",
       " (13925, 'my', 1),\n",
       " (14240, 'no', 1),\n",
       " (14557, 'of', 3),\n",
       " (14809, 'out', 1),\n",
       " (15448, 'perfectly', 1),\n",
       " (15993, 'point', 1),\n",
       " (16469, 'process', 1),\n",
       " (17857, 'return', 1),\n",
       " (18782, 'seemed', 1),\n",
       " (18923, 'set', 1),\n",
       " (19573, 'so', 1),\n",
       " (21228, 'the', 4),\n",
       " (21314, 'this', 1),\n",
       " (21529, 'to', 1),\n",
       " (22410, 'uniform', 1),\n",
       " (23320, 'wall', 1),\n",
       " (23565, 'whence', 1),\n",
       " (23793, 'without', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(binary=False)\n",
    "X_train_cv = count_vect.fit_transform(train['text'])\n",
    "\n",
    "one_hot_vector = count_vect.transform([first_sentence]).toarray()\n",
    "indices = np.where(np.any(one_hot_vector!=0, axis=0))[0]\n",
    "index_to_word = {index : word for word, index in count_vect.vocabulary_.items()}\n",
    "\n",
    "[(ind, index_to_word[ind], one_hot_vector[0, ind]) for ind in indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8553e30c-b37e-44a4-a8ca-b7a2c71ea00a",
   "metadata": {},
   "source": [
    "We can see that now the value for 'the' is 4. It doesn't only show that this article is present in the sentence, but also indicate how many times it occurs in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1201b65-a554-45e3-90bf-95b2e4c5f124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = count_vect.build_tokenizer()\n",
    "tokenized_sentence = tokenizer(first_sentence.lower())\n",
    "tokenized_sentence.count('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1766375f-9b1e-4f6d-9bd4-d96f33e295c6",
   "metadata": {},
   "source": [
    "##### 3.1.1 Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c80bf-6cc5-47e8-b0f0-264877fa5843",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ba2f23b-91ca-4367-9a36-05ea3f47c05f",
   "metadata": {},
   "source": [
    "##### 3.1.2 Disadvantages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9291e613-7c5c-46b4-b528-f7e5d4bf6eba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803cd6c1-4f03-4224-b11d-44fdbf769974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
