{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6962a270-9e28-4cd3-8de6-e12decff3fdf",
   "metadata": {},
   "source": [
    "# a. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610dfbb8-ea03-4aaf-89bc-fbb8ecec15d1",
   "metadata": {},
   "source": [
    "$$\\normalsize J_{naive-softmax}(v_c;o;U) = -log(P(O = o|C = c))$$(2)  \n",
    "We can view this loss as the cross-entropy between the true distribution $y$ and the predicted distribution $\\hat{y}$. Here, both $y$ and $\\hat{y}$ are vectors with length equal to the number of words in the vocabulary. Furthermore, the $k^{th}$ entry in these vectors indicates the conditional probability of the $k^{th}$ word being an *outside word* for the given *c*. The true empirical distribution $y$ is a one-hot vector with a $1$ for the true outside word $o$, and $0$ everywhere else."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27febda-849e-4a1b-8145-aa7ca62ae8c3",
   "metadata": {},
   "source": [
    "Show that the naive-softmax loss given in Equation (2) is the same as the cross-entropy loss between $y$ and $\\hat{y}$; i.e., show that \n",
    "$$ \\normalsize -\\sum_{w \\in Vocab}{y_w log(\\hat{y}_w)} = -log(\\hat{y}_o)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239b0772-cd0f-4e32-8cdc-54b208fb7279",
   "metadata": {},
   "source": [
    "Solution: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c420bd6-160a-458d-9c5f-40e579cfa822",
   "metadata": {},
   "source": [
    "$$ \\normalsize -\\sum_{w \\in Vocab}{y_w log(\\hat{y}_w)} = - \\left(y_{w_1}log(\\hat{y}_{w_1}) + y_{w_2}log(\\hat{y}_{w_2}) + ... + y_{w_k}log(\\hat{y}_{w_k}) + ... + y_{w_{|V|}}log(\\hat{y}_{w_{|V|}})\\right) = -y_{w_k}log(\\hat{y}_{w_k}) = -log(\\hat{y}_{w_k}) = -log(\\hat{y}_{o})$$\n",
    "Because, $y_{w_i}=0$ for $i \\neq k$ and $y_{w_k}=1$ due to the nature of the hot-vectors, where $k$ is a position of outside word $o$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdca1d4d-8577-4591-9b92-d2fe30faa9ff",
   "metadata": {},
   "source": [
    "# b. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bc93dc-23a1-401a-ad95-ee6d2eed6901",
   "metadata": {},
   "source": [
    "$$\\normalsize J_{naive-softmax}(v_c;o;U) = -log(P(O = o|C = c)) = - log\\left(\\frac{exp(u^{T}_{o}v_{c})}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\right)$$\n",
    "\n",
    "$$ \\normalsize \\frac{\\partial }{\\partial v_{c}}J_{naive-softmax}(v_c;o;U) =\n",
    "\\frac{\\partial }{\\partial v_{c}}\\left(- log\\left(\\frac{exp(u^{T}_{o}v_{c})}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\right)\\right)=\n",
    "-\\frac{\\partial }{\\partial v_{c}}log\\left(exp(u^{T}_{o}v_{c})\\right) + \\frac{\\partial }{\\partial v_{c}}log\\left(\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}\\right)$$  \n",
    "\n",
    "$\\normalsize -\\frac{\\partial }{\\partial v_{c}}log\\left(exp(u^{T}_{o}v_{c})\\right) = -\\frac{\\partial }{\\partial v_{c}}u^{T}_{o}v_{c} = -u_{o}$  \n",
    "\n",
    "$\\normalsize \\frac{\\partial }{\\partial v_{c}}log\\left(\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}\\right) = \\frac{1}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\frac{\\partial }{\\partial v_{c}}\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})} = \\frac{1}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\sum_{w=1}^{V}{\\frac{\\partial }{\\partial v_{c}}exp(u^{T}_{w}v_{c})}=\\frac{\\sum_{x=1}^{V}{u_{x}exp(u^{T}_{x}v_{c})}}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}=\\sum_{x=1}^{V}{\\frac{exp(u^{T}_{x}v_{c})u_{x}}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}}=\\sum_{x=1}^{V}u_{x}P(x|c)$   \n",
    "$$ \\normalsize \\frac{\\partial }{\\partial v_{c}}J_{naive-softmax}(v_c;o;U) = -u_{o} + \\sum_{x=1}^{V}u_{x}P(x|c) = -u_{o} + \\sum_{x=1}^{V}\\hat{y}_{x}u_{x} = U\\hat{y} - y $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6fc6d9-2954-426f-8b7f-6eb9223a3524",
   "metadata": {},
   "source": [
    "# c. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb070ee9-c1f1-4775-aac8-6d8cf9d3dbc1",
   "metadata": {},
   "source": [
    "$w = o$:\n",
    "$$ \\normalsize \\frac{\\partial }{\\partial u_{o}}J_{naive-softmax}(v_c;o;U) =\n",
    "\\frac{\\partial }{\\partial u_{o}}\\left(- log\\left(\\frac{exp(u^{T}_{o}v_{c})}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\right)\\right)=\n",
    "-\\frac{\\partial }{\\partial u_{o}}log\\left(exp(u^{T}_{o}v_{c})\\right) + \\frac{\\partial }{\\partial u_{o}}log\\left(\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}\\right)$$  \n",
    "\n",
    "$\\normalsize -\\frac{\\partial }{\\partial u_{o}}log\\left(exp(u^{T}_{o}v_{c})\\right) = -\\frac{\\partial }{\\partial u_{o}}u^{T}_{o}v_{c} = -v_{c}$  \n",
    "\n",
    "$\\normalsize \\frac{\\partial }{\\partial u_{o}}log\\left(\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}\\right) = \n",
    "\\frac{1}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\frac{\\partial }{\\partial u_{o}}\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})} = \n",
    "\\frac{1}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\frac{\\partial }{\\partial u_{o}}\\left(\\sum_{w=1, w \\neq k}^{V}{exp(u^{T}_{w}v_{c})} + exp(u^{T}_{o}v_{c})\\right) = \n",
    "\\frac{1}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\frac{\\partial }{\\partial u_{o}}\\left(exp(u^{T}_{o}v_{c})\\right) = \n",
    "\\frac{exp(u^{T}_{o}v_{c})v_{c}}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}} =\n",
    "P(o|c)v_{c} = \n",
    "\\hat{y}_{o}v_{c}$\n",
    "\n",
    "For $w = o$:\n",
    "$$ \\normalsize \\frac{\\partial }{\\partial u_{w}}J_{naive-softmax}(v_c;o;U) = -v_{c} + \\hat{y}_{w}v_{c} = v_{c}(\\hat{y}_{w} - 1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed6f734-2eab-4249-81c1-ca68ca582af5",
   "metadata": {},
   "source": [
    "$w \\neq o$, let $w = l$ (arbitrary), $l \\ in [1, |V|]$:  \n",
    "\n",
    "$$ \\normalsize \\frac{\\partial }{\\partial u_{l}}J_{naive-softmax}(v_c;o;U) =\n",
    "\\frac{\\partial }{\\partial u_{l}}\\left(- log\\left(\\frac{exp(u^{T}_{o}v_{c})}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\right)\\right)=\n",
    "-\\frac{\\partial }{\\partial u_{l}}log\\left(exp(u^{T}_{o}v_{c})\\right) + \\frac{\\partial }{\\partial u_{l}}log\\left(\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}\\right)$$  \n",
    "\n",
    "$\\normalsize -\\frac{\\partial }{\\partial u_{l}}log\\left(exp(u^{T}_{o}v_{c})\\right) = -\\frac{\\partial }{\\partial u_{l}}u^{T}_{o}v_{c} = 0$  \n",
    "\n",
    "$\\normalsize \\frac{\\partial }{\\partial u_{l}}log\\left(\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}\\right) = \n",
    "\\frac{1}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\frac{\\partial }{\\partial u_{l}}\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})} = \n",
    "\\frac{1}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\frac{\\partial }{\\partial u_{l}}\\left(\\sum_{w=1, w \\neq l}^{V}{exp(u^{T}_{w}v_{c})} + exp(u^{T}_{l}v_{c})\\right) = \n",
    "\\frac{1}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\frac{\\partial }{\\partial u_{l}}\\left(exp(u^{T}_{l}v_{c})\\right) = \n",
    "\\frac{exp(u^{T}_{l}v_{c})v_{c}}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}} =\n",
    "P(l|c)v_{c} = \n",
    "\\hat{y}_{l}v_{c}$\n",
    "\n",
    "For $w \\neq o$:\n",
    "$$ \\normalsize \\frac{\\partial }{\\partial u_{w}}J_{naive-softmax}(v_c;o;U) = \\hat{y}_{w}v_{c} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62fe5c3-6815-423a-a8b5-8f2e4920f081",
   "metadata": {},
   "source": [
    "# d."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b60d8d4-2b46-4740-bb47-23f8bf8a0131",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
