{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6962a270-9e28-4cd3-8de6-e12decff3fdf",
   "metadata": {},
   "source": [
    "# a. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610dfbb8-ea03-4aaf-89bc-fbb8ecec15d1",
   "metadata": {},
   "source": [
    "$$\\large J_{naive-softmax}(v_c;o;U) = -log(P(O = o|C = c))$$(2)  \n",
    "We can view this loss as the cross-entropy between the true distribution $y$ and the predicted distribution $\\hat{y}$. Here, both $y$ and $\\hat{y}$ are vectors with length equal to the number of words in the vocabulary. Furthermore, the $k^{th}$ entry in these vectors indicates the conditional probability of the $k^{th}$ word being an *outside word* for the given *c*. The true empirical distribution $y$ is a one-hot vector with a $1$ for the true outside word $o$, and $0$ everywhere else."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27febda-849e-4a1b-8145-aa7ca62ae8c3",
   "metadata": {},
   "source": [
    "Show that the naive-softmax loss given in Equation (2) is the same as the cross-entropy loss between $y$ and $\\hat{y}$; i.e., show that \n",
    "$$ \\large -\\sum_{w \\in Vocab}{y_w log(\\hat{y}_w)} = -log(\\hat{y}_o)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239b0772-cd0f-4e32-8cdc-54b208fb7279",
   "metadata": {},
   "source": [
    "Solution: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c420bd6-160a-458d-9c5f-40e579cfa822",
   "metadata": {},
   "source": [
    "$$ \\normalsize -\\sum_{w \\in Vocab}{y_w log(\\hat{y}_w)} = - \\left(y_{w_1}log(\\hat{y}_{w_1}) + y_{w_2}log(\\hat{y}_{w_2}) + ... + y_{w_k}log(\\hat{y}_{w_k}) + ... + y_{w_{|V|}}log(\\hat{y}_{w_{|V|}})\\right) = -y_{w_k}log(\\hat{y}_{w_k}) = -log(\\hat{y}_{w_k}) = -log(\\hat{y}_{o})$$\n",
    "Because, $y_{w_i}=0$ for $i \\neq k$ and $y_{w_k}=1$ due to the nature of the hot-vectors, where $k$ is a position of outside word $o$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2272bb04-d981-4ff8-b7d7-156f244c2c49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
