{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6962a270-9e28-4cd3-8de6-e12decff3fdf",
   "metadata": {},
   "source": [
    "# a. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610dfbb8-ea03-4aaf-89bc-fbb8ecec15d1",
   "metadata": {},
   "source": [
    "$$\\normalsize J_{naive-softmax}(v_c;o;U) = -log(P(O = o|C = c))$$(2)  \n",
    "We can view this loss as the cross-entropy between the true distribution $y$ and the predicted distribution $\\hat{y}$. Here, both $y$ and $\\hat{y}$ are vectors with length equal to the number of words in the vocabulary. Furthermore, the $k^{th}$ entry in these vectors indicates the conditional probability of the $k^{th}$ word being an *outside word* for the given *c*. The true empirical distribution $y$ is a one-hot vector with a $1$ for the true outside word $o$, and $0$ everywhere else."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27febda-849e-4a1b-8145-aa7ca62ae8c3",
   "metadata": {},
   "source": [
    "Show that the naive-softmax loss given in Equation (2) is the same as the cross-entropy loss between $y$ and $\\hat{y}$; i.e., show that \n",
    "$$ \\normalsize -\\sum_{w \\in Vocab}{y_w log(\\hat{y}_w)} = -log(\\hat{y}_o)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239b0772-cd0f-4e32-8cdc-54b208fb7279",
   "metadata": {},
   "source": [
    "Solution: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c420bd6-160a-458d-9c5f-40e579cfa822",
   "metadata": {},
   "source": [
    "$$ \\normalsize -\\sum_{w \\in Vocab}{y_w log(\\hat{y}_w)} = - \\left(y_{w_1}log(\\hat{y}_{w_1}) + y_{w_2}log(\\hat{y}_{w_2}) + ... + y_{w_k}log(\\hat{y}_{w_k}) + ... + y_{w_{|V|}}log(\\hat{y}_{w_{|V|}})\\right) = -y_{w_k}log(\\hat{y}_{w_k}) = -log(\\hat{y}_{w_k}) = -log(\\hat{y}_{o})$$\n",
    "Because, $y_{w_i}=0$ for $i \\neq k$ and $y_{w_k}=1$ due to the nature of the hot-vectors, where $k$ is a position of outside word $o$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdca1d4d-8577-4591-9b92-d2fe30faa9ff",
   "metadata": {},
   "source": [
    "# b. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bc93dc-23a1-401a-ad95-ee6d2eed6901",
   "metadata": {},
   "source": [
    "$$\\normalsize J_{naive-softmax}(v_c;o;U) = -log(P(O = o|C = c)) = - log\\left(\\frac{exp(u^{T}_{o}v_{c})}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\right)$$\n",
    "\n",
    "$$ \\normalsize \\frac{\\partial }{\\partial v_{c}}J_{naive-softmax}(v_c;o;U) =\n",
    "\\frac{\\partial }{\\partial v_{c}}\\left(- log\\left(\\frac{exp(u^{T}_{o}v_{c})}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\right)\\right)=\n",
    "-\\frac{\\partial }{\\partial v_{c}}log\\left(exp(u^{T}_{o}v_{c})\\right) + \\frac{\\partial }{\\partial v_{c}}log\\left(\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}\\right)$$  \n",
    "\n",
    "$\\normalsize -\\frac{\\partial }{\\partial v_{c}}log\\left(exp(u^{T}_{o}v_{c})\\right) = -\\frac{\\partial }{\\partial v_{c}}u^{T}_{o}v_{c} = -u_{o}$  \n",
    "\n",
    "$\\normalsize \\frac{\\partial }{\\partial v_{c}}log\\left(\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}\\right) = \\frac{1}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\frac{\\partial }{\\partial v_{c}}\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})} = \\frac{1}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\sum_{w=1}^{V}{\\frac{\\partial }{\\partial v_{c}}exp(u^{T}_{w}v_{c})}=\\frac{\\sum_{x=1}^{V}{u_{x}exp(u^{T}_{x}v_{c})}}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}=\\sum_{x=1}^{V}{\\frac{exp(u^{T}_{x}v_{c})u_{x}}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}}=\\sum_{x=1}^{V}u_{x}P(x|c)$   \n",
    "$$ \\normalsize \\frac{\\partial }{\\partial v_{c}}J_{naive-softmax}(v_c;o;U) = -u_{o} + \\sum_{x=1}^{V}u_{x}P(x|c) = -u_{o} + \\sum_{x=1}^{V}\\hat{y}_{x}u_{x} = - Uy + U\\hat{y} = U(\\hat{y} - y) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6fc6d9-2954-426f-8b7f-6eb9223a3524",
   "metadata": {},
   "source": [
    "# c. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb070ee9-c1f1-4775-aac8-6d8cf9d3dbc1",
   "metadata": {},
   "source": [
    "$w = o$:\n",
    "$$ \\normalsize \\frac{\\partial }{\\partial u_{o}}J_{naive-softmax}(v_c;o;U) =\n",
    "\\frac{\\partial }{\\partial u_{o}}\\left(- log\\left(\\frac{exp(u^{T}_{o}v_{c})}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\right)\\right)=\n",
    "-\\frac{\\partial }{\\partial u_{o}}log\\left(exp(u^{T}_{o}v_{c})\\right) + \\frac{\\partial }{\\partial u_{o}}log\\left(\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}\\right)$$  \n",
    "\n",
    "$\\normalsize -\\frac{\\partial }{\\partial u_{o}}log\\left(exp(u^{T}_{o}v_{c})\\right) = -\\frac{\\partial }{\\partial u_{o}}u^{T}_{o}v_{c} = -v_{c}$  \n",
    "\n",
    "$\\normalsize \\frac{\\partial }{\\partial u_{o}}log\\left(\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}\\right) = \n",
    "\\frac{1}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\frac{\\partial }{\\partial u_{o}}\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})} = \n",
    "\\frac{1}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\frac{\\partial }{\\partial u_{o}}\\left(\\sum_{w=1, w \\neq k}^{V}{exp(u^{T}_{w}v_{c})} + exp(u^{T}_{o}v_{c})\\right) = \n",
    "\\frac{1}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\frac{\\partial }{\\partial u_{o}}\\left(exp(u^{T}_{o}v_{c})\\right) = \n",
    "\\frac{exp(u^{T}_{o}v_{c})v_{c}}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}} =\n",
    "P(o|c)v_{c} = \n",
    "\\hat{y}_{o}v_{c}$\n",
    "\n",
    "For $w = o$:\n",
    "$$ \\normalsize \\frac{\\partial }{\\partial u_{w}}J_{naive-softmax}(v_c;o;U) = -v_{c} + \\hat{y}_{w}v_{c} = v_{c}(\\hat{y}_{w} - 1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed6f734-2eab-4249-81c1-ca68ca582af5",
   "metadata": {},
   "source": [
    "$w \\neq o$, let $w = l$ (arbitrary), $l \\ in [1, |V|]$:  \n",
    "\n",
    "\n",
    "$$ \\normalsize \\frac{\\partial }{\\partial u_{l}}J_{naive-softmax}(v_c;o;U) =\n",
    "\\frac{\\partial }{\\partial u_{l}}\\left(- log\\left(\\frac{exp(u^{T}_{o}v_{c})}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\right)\\right)=\n",
    "-\\frac{\\partial }{\\partial u_{l}}log\\left(exp(u^{T}_{o}v_{c})\\right) + \\frac{\\partial }{\\partial u_{l}}log\\left(\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}\\right)$$  \n",
    "\n",
    "$\\normalsize -\\frac{\\partial }{\\partial u_{l}}log\\left(exp(u^{T}_{o}v_{c})\\right) = -\\frac{\\partial }{\\partial u_{l}}u^{T}_{o}v_{c} = 0$  \n",
    "\n",
    "$\\normalsize \\frac{\\partial }{\\partial u_{l}}log\\left(\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}\\right) = \n",
    "\\frac{1}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\frac{\\partial }{\\partial u_{l}}\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})} = \n",
    "\\frac{1}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\frac{\\partial }{\\partial u_{l}}\\left(\\sum_{w=1, w \\neq l}^{V}{exp(u^{T}_{w}v_{c})} + exp(u^{T}_{l}v_{c})\\right) = \n",
    "\\frac{1}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\frac{\\partial }{\\partial u_{l}}\\left(exp(u^{T}_{l}v_{c})\\right) = \n",
    "\\frac{exp(u^{T}_{l}v_{c})v_{c}}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}} =\n",
    "P(l|c)v_{c} = \n",
    "\\hat{y}_{l}v_{c}$\n",
    "\n",
    "For $w \\neq o$:\n",
    "$$ \\normalsize \\frac{\\partial }{\\partial u_{w}}J_{naive-softmax}(v_c;o;U) = \\hat{y}_{w}v_{c} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b6b1f1-9a69-4426-b5cf-e79dfef88265",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial }{\\partial u_{w}}J_{naive-softmax}(v_c;o;U) = \n",
    "\\left[\n",
    "\\begin{array}{ll}\n",
    "    v_{c}\\hat{y}_{w} & w \\neq o \\\\\n",
    "    v_{c}(\\hat{y}_{w} - 1) & w = o\n",
    "\\end{array}\n",
    "\\right .\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62fe5c3-6815-423a-a8b5-8f2e4920f081",
   "metadata": {},
   "source": [
    "# d."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b60d8d4-2b46-4740-bb47-23f8bf8a0131",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a24cc5c-63d4-49f5-b222-599ecaa9ff2c",
   "metadata": {},
   "source": [
    "# e."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368f29ed-3ec7-489d-95eb-5539dc2f8e21",
   "metadata": {},
   "source": [
    "$\\normalsize \\sigma(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{1 + e^x}$  \n",
    "\n",
    "$\\normalsize \\sigma'(x) = \\frac{(e^x)'(1 + e^x) - e^x(1 + e^x)'}{(1 + e^x)^2} = \n",
    "\\frac{e^x(1 + e^x)}{(1 + e^x)^2} - \\frac{e^x}{(1 + e^x)}\\frac{e^x}{(1 + e^x)} =\n",
    "\\frac{e^x}{(1 + e^x)} - \\frac{e^x}{(1 + e^x)}\\frac{e^x}{(1 + e^x)} = \n",
    "\\sigma(x) - \\sigma(x)\\sigma(x) = \\sigma(x)(1 - \\sigma(x))$\n",
    "\n",
    "Remark: $\\normalsize \\sigma(-x) = 1 - \\sigma(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002cc323-e0f1-4fab-862a-406238e094a3",
   "metadata": {},
   "source": [
    "# f."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3dfd80-b4fd-4001-b9b6-be9267dd451c",
   "metadata": {},
   "source": [
    "$$\\normalsize J_{neg-sample}(v_c;o;U) = -log\\left(\\sigma(u^{T}_{o}v_{c})\\right) - \\sum_{k=1}^{K}{log\\left(\\sigma(-u^{T}_{k}v_{c})\\right)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcbdb44-be93-4ba0-82d7-9039be8f7d2d",
   "metadata": {},
   "source": [
    "1. $\\normalsize \\frac{\\partial }{\\partial v_{c}}J_{neg-sample}(v_c;o;U)$:\n",
    "\n",
    "    $\\normalsize \\frac{\\partial }{\\partial v_{c}}\\left(-log\\left(\\sigma(u^{T}_{o}v_{c})\\right)\\right) = \n",
    "    -\\frac{1}{\\sigma(u^{T}_{o}v_{c})}\\sigma(u^{T}_{o}v_{c})(1 - \\sigma(u^{T}_{o}v_{c}))\\frac{\\partial }{\\partial v_{c}}u^{T}_{o}v_{c} = \n",
    "    (\\sigma(u^{T}_{o}v_{c}) - 1)u_{o}$  \n",
    "\n",
    "    $\\normalsize \\frac{\\partial }{\\partial v_{c}}\\left(-\\sum_{k=1}^{K}{log\\left(\\sigma(-u^{T}_{k}v_{c})\\right)}\\right) = \n",
    "    -\\sum_{k=1}^{K}{\\frac{\\partial }{\\partial v_{c}}log\\left(\\sigma(-u^{T}_{k}v_{c})\\right)} = \n",
    "    -\\sum_{k=1}^{K}{(-u_{k})(1 - \\sigma(-u^{T}_{k}v_{c}))} = \n",
    "    \\sum_{k=1}^{K}{u_{k}\\sigma(u^{T}_{k}v_{c})}$\n",
    "\n",
    "$$\\normalsize \\frac{\\partial }{\\partial v_{c}}J_{neg-sample}(v_c;o;U) = (\\sigma(u^{T}_{o}v_{c}) - 1)u_{o} + \\sum_{k=1}^{K}{u_{k}\\sigma(u^{T}_{k}v_{c})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a343cc3-0d96-406d-bd0d-31869cfb34d5",
   "metadata": {},
   "source": [
    "2. $\\normalsize \\frac{\\partial }{\\partial u_{k}}J_{neg-sample}(v_c;o;U)$:  \n",
    "    Note that $o \\notin {w_1; ... ;w_K}$ and where $k \\in [1;K]$  \n",
    "\n",
    "    $\\normalsize \\frac{\\partial }{\\partial u_{k}}\\left(-log\\left(\\sigma(u^{T}_{o}v_{c})\\right)\\right) = 0$  \n",
    "\n",
    "    $\\normalsize \\frac{\\partial }{\\partial u_{k}}\\left(-\\sum_{k=1}^{K}{log\\left(\\sigma(-u^{T}_{k}v_{c})\\right)}\\right) = \n",
    "    -{\\frac{\\partial }{\\partial u_{k}}log\\left(\\sigma(-u^{T}_{k}v_{c})\\right)} = \n",
    "    -{(-v_{c})(1 - \\sigma(-u^{T}_{k}v_{c}))} = \n",
    "    v_{c}\\sigma(u^{T}_{k}v_{c})$\n",
    "\n",
    "$$\\normalsize \\frac{\\partial }{\\partial u_{k}}J_{neg-sample}(v_c;o;U) = v_{c}\\sigma(u^{T}_{k}v_{c})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43069e8a-2c2c-4d74-a407-1be41fc608a3",
   "metadata": {},
   "source": [
    "2. $\\normalsize \\frac{\\partial }{\\partial u_{o}}J_{neg-sample}(v_c;o;U)$:  \n",
    "    Note that $o \\notin {w_1; ... ;w_K}$$  \n",
    "\n",
    "    $\\normalsize \\frac{\\partial }{\\partial u_{o}}\\left(-log\\left(\\sigma(u^{T}_{o}v_{c})\\right)\\right) = \n",
    "    -{\\frac{\\partial }{\\partial u_{o}}log\\left(\\sigma(u^{T}_{o}v_{c})\\right)} = \n",
    "    -(v_{c})(1 - \\sigma(u^{T}_{o}v_{c})) = \n",
    "    v_{c}(\\sigma(u^{T}_{o}v_{c}) - 1)$  \n",
    "\n",
    "    $\\normalsize \\frac{\\partial }{\\partial u_{o}}\\left(-\\sum_{k=1}^{K}{log\\left(\\sigma(-u^{T}_{k}v_{c})\\right)}\\right) = 0$\n",
    "\n",
    "$$\\normalsize \\frac{\\partial }{\\partial u_{o}}J_{neg-sample}(v_c;o;U) = v_{c}(\\sigma(u^{T}_{o}v_{c}) - 1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d2eacc-b060-4d5e-bce3-1598fbf8103e",
   "metadata": {},
   "source": [
    "So,   \n",
    "\\begin{align}\n",
    "\\frac{\\partial }{\\partial u_{k}}J_{neg-sample}(v_c;o;U) = \n",
    "\\left[\n",
    "\\begin{array}{ll}\n",
    "    v_{c}\\sigma(u^{T}_{k}v_{c}) & k \\neq o \\\\\n",
    "    v_{c}(\\sigma(u^{T}_{k}v_{c}) - 1) & k = o\n",
    "\\end{array}\n",
    "\\right .\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a789c8be-4fda-4187-93e8-c91cc9b59ab5",
   "metadata": {},
   "source": [
    "Q: *Why negative-sample loss function is much more efficient to compute than the naive-softmax loss?*  \n",
    "A:  Any update we do or evaluation of the naive-softmax function would take $O(|V|)$ time which if we recall is in the millions. For every training step using negative samples, instead of looping over the entire vocabulary, we can just sample several negative examples!\n",
    "\n",
    "$$ \\normalsize \\frac{\\partial }{\\partial v_{c}}J_{naive-softmax}(v_c;o;U) = U(\\hat{y} - y) $$\n",
    "\\begin{align}\n",
    "\\frac{\\partial }{\\partial u_{w}}J_{naive-softmax}(v_c;o;U) = \n",
    "\\left[\n",
    "\\begin{array}{ll}\n",
    "    v_{c}\\hat{y}_{w} & w \\neq o \\\\\n",
    "    v_{c}(\\hat{y}_{w} - 1) & w = o\n",
    "\\end{array}\n",
    "\\right .\n",
    "\\end{align}\n",
    "And  \n",
    "$$\\normalsize \\frac{\\partial }{\\partial v_{c}}J_{neg-sample}(v_c;o;U) = (\\sigma(u^{T}_{o}v_{c}) - 1)u_{o} + \\sum_{k=1}^{K}{u_{k}\\sigma(u^{T}_{k}v_{c})} $$\n",
    "\\begin{align}\n",
    "\\frac{\\partial }{\\partial u_{k}}J_{neg-sample}(v_c;o;U) = \n",
    "\\left[\n",
    "\\begin{array}{ll}\n",
    "    v_{c}\\sigma(u^{T}_{k}v_{c}) & k \\neq o \\\\\n",
    "    v_{c}(\\sigma(u^{T}_{k}v_{c}) - 1) & k = o\n",
    "\\end{array}\n",
    "\\right .\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd181ba-7fa2-4ba5-af21-098bccd28290",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
