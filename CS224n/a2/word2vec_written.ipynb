{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6962a270-9e28-4cd3-8de6-e12decff3fdf",
   "metadata": {},
   "source": [
    "# a. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610dfbb8-ea03-4aaf-89bc-fbb8ecec15d1",
   "metadata": {},
   "source": [
    "**Question**: Show that the naive-softmax loss given in Equation $$\\normalsize J_{naive-softmax}(v_c;o;U) = -log(P(O = o|C = c))$$ is the same as the cross-entropy loss\n",
    "between $y$ and $\\hat{y}$; i.e., show that\n",
    "$$ \\normalsize -\\sum_{w \\in Vocab}{y_w log(\\hat{y}_w)} = -log(\\hat{y}_o)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c420bd6-160a-458d-9c5f-40e579cfa822",
   "metadata": {},
   "source": [
    "**Answer**:  \n",
    "\n",
    "$$ \\normalsize -\\sum_{w \\in Vocab}{y_w log(\\hat{y}_w)} = - \\left(y_{w_1}log(\\hat{y}_{w_1}) + y_{w_2}log(\\hat{y}_{w_2}) + ... + y_{w_k}log(\\hat{y}_{w_k}) + ... + y_{w_{|V|}}log(\\hat{y}_{w_{|V|}})\\right) = -y_{w_k}log(\\hat{y}_{w_k}) = -log(\\hat{y}_{w_k}) = -log(\\hat{y}_{o})$$\n",
    "Because, $y_{w_i}=0$ for $i \\neq k$ and $y_{w_k}=1$ due to the nature of the hot-vectors, where $k$ is a position of outside word $o$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdca1d4d-8577-4591-9b92-d2fe30faa9ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "# b. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2433e613-ad9f-416e-9068-22b7f324fefb",
   "metadata": {},
   "source": [
    "**Question**: Compute the partial derivative of $J_{naive-softmax}(v_c, o, U)$ with respect to $v_c$. Please write your answer in terms of $y$, $\\hat{y}$, and $U$. Note that in this course, we expect your final answers to follow the shape convention. This means that the partial derivative of any function $f(x)$ with respect to $x$ should have the same shape as $x$. For this subpart, please present your answer in vectorized form. In particular, you may not refer to specific elements of $y$, $\\hat{y}$, and $U$ in your final answer (such as $y_1, y_2, ...$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bc93dc-23a1-401a-ad95-ee6d2eed6901",
   "metadata": {},
   "source": [
    "**Answer**:  \n",
    "\n",
    "$$\\normalsize J_{naive-softmax}(v_c;o;U) = -log(P(O = o|C = c)) = - log\\left(\\frac{exp(u^{T}_{o}v_{c})}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\right)$$\n",
    "\n",
    "$$ \\normalsize \\frac{\\partial }{\\partial v_{c}}J_{naive-softmax}(v_c;o;U) =\n",
    "\\frac{\\partial }{\\partial v_{c}}\\left(- log\\left(\\frac{exp(u^{T}_{o}v_{c})}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\right)\\right)=\n",
    "-\\frac{\\partial }{\\partial v_{c}}log\\left(exp(u^{T}_{o}v_{c})\\right) + \\frac{\\partial }{\\partial v_{c}}log\\left(\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}\\right)$$  \n",
    "\n",
    "$\\normalsize -\\frac{\\partial }{\\partial v_{c}}log\\left(exp(u^{T}_{o}v_{c})\\right) = -\\frac{\\partial }{\\partial v_{c}}u^{T}_{o}v_{c} = -u_{o}$  \n",
    "\n",
    "$\\normalsize \\frac{\\partial }{\\partial v_{c}}log\\left(\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}\\right) = \\frac{1}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\frac{\\partial }{\\partial v_{c}}\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})} = \\frac{1}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\sum_{w=1}^{V}{\\frac{\\partial }{\\partial v_{c}}exp(u^{T}_{w}v_{c})}=\\frac{\\sum_{x=1}^{V}{u_{x}exp(u^{T}_{x}v_{c})}}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}=\\sum_{x=1}^{V}{\\frac{exp(u^{T}_{x}v_{c})u_{x}}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}}=\\sum_{x=1}^{V}u_{x}P(x|c)$   \n",
    "$$ \\normalsize \\frac{\\partial }{\\partial v_{c}}J_{naive-softmax}(v_c;o;U) = -u_{o} + \\sum_{x=1}^{V}u_{x}P(x|c) = -u_{o} + \\sum_{x=1}^{V}\\hat{y}_{x}u_{x} = - Uy + U\\hat{y} = U(\\hat{y} - y) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6fc6d9-2954-426f-8b7f-6eb9223a3524",
   "metadata": {},
   "source": [
    "# c. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ccc55c-7e1d-46c3-b7c5-d23c04cb7f5d",
   "metadata": {},
   "source": [
    "**Question**: Compute the partial derivatives of $J_{naive-softmax}(v_c, o, U)$ with respect to each of the *outside* word vectors, $u_w$'s. There will be two cases: when $w = o$, the true *outside* word vector, and $w \\neq o$, for all other words. Please write your answer in terms of $y$, $\\hat{y}$, and $v_c$. In this subpart, you may use specific elements within these terms as well, such as $(y_1; y_2; ...)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb070ee9-c1f1-4775-aac8-6d8cf9d3dbc1",
   "metadata": {},
   "source": [
    "**Answer**:  \n",
    "\n",
    "$w = o$:\n",
    "$$ \\normalsize \\frac{\\partial }{\\partial u_{o}}J_{naive-softmax}(v_c;o;U) =\n",
    "\\frac{\\partial }{\\partial u_{o}}\\left(- log\\left(\\frac{exp(u^{T}_{o}v_{c})}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\right)\\right)=\n",
    "-\\frac{\\partial }{\\partial u_{o}}log\\left(exp(u^{T}_{o}v_{c})\\right) + \\frac{\\partial }{\\partial u_{o}}log\\left(\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}\\right)$$  \n",
    "\n",
    "$\\normalsize -\\frac{\\partial }{\\partial u_{o}}log\\left(exp(u^{T}_{o}v_{c})\\right) = -\\frac{\\partial }{\\partial u_{o}}u^{T}_{o}v_{c} = -v_{c}$  \n",
    "\n",
    "$\\normalsize \\frac{\\partial }{\\partial u_{o}}log\\left(\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}\\right) = \n",
    "\\frac{1}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\frac{\\partial }{\\partial u_{o}}\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})} = \n",
    "\\frac{1}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\frac{\\partial }{\\partial u_{o}}\\left(\\sum_{w=1, w \\neq k}^{V}{exp(u^{T}_{w}v_{c})} + exp(u^{T}_{o}v_{c})\\right) = \n",
    "\\frac{1}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\frac{\\partial }{\\partial u_{o}}\\left(exp(u^{T}_{o}v_{c})\\right) = \n",
    "\\frac{exp(u^{T}_{o}v_{c})v_{c}}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}} =\n",
    "P(o|c)v_{c} = \n",
    "\\hat{y}_{o}v_{c}$\n",
    "\n",
    "For $w = o$:\n",
    "$$ \\normalsize \\frac{\\partial }{\\partial u_{w}}J_{naive-softmax}(v_c;o;U) = -v_{c} + \\hat{y}_{w}v_{c} = v_{c}(\\hat{y}_{w} - 1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed6f734-2eab-4249-81c1-ca68ca582af5",
   "metadata": {},
   "source": [
    "$w \\neq o$, let $w = l$ (arbitrary), $l \\ in [1, |V|]$:  \n",
    "\n",
    "\n",
    "$$ \\normalsize \\frac{\\partial }{\\partial u_{l}}J_{naive-softmax}(v_c;o;U) =\n",
    "\\frac{\\partial }{\\partial u_{l}}\\left(- log\\left(\\frac{exp(u^{T}_{o}v_{c})}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\right)\\right)=\n",
    "-\\frac{\\partial }{\\partial u_{l}}log\\left(exp(u^{T}_{o}v_{c})\\right) + \\frac{\\partial }{\\partial u_{l}}log\\left(\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}\\right)$$  \n",
    "\n",
    "$\\normalsize -\\frac{\\partial }{\\partial u_{l}}log\\left(exp(u^{T}_{o}v_{c})\\right) = -\\frac{\\partial }{\\partial u_{l}}u^{T}_{o}v_{c} = 0$  \n",
    "\n",
    "$\\normalsize \\frac{\\partial }{\\partial u_{l}}log\\left(\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}\\right) = \n",
    "\\frac{1}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\frac{\\partial }{\\partial u_{l}}\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})} = \n",
    "\\frac{1}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\frac{\\partial }{\\partial u_{l}}\\left(\\sum_{w=1, w \\neq l}^{V}{exp(u^{T}_{w}v_{c})} + exp(u^{T}_{l}v_{c})\\right) = \n",
    "\\frac{1}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}}\\frac{\\partial }{\\partial u_{l}}\\left(exp(u^{T}_{l}v_{c})\\right) = \n",
    "\\frac{exp(u^{T}_{l}v_{c})v_{c}}{\\sum_{w=1}^{V}{exp(u^{T}_{w}v_{c})}} =\n",
    "P(l|c)v_{c} = \n",
    "\\hat{y}_{l}v_{c}$\n",
    "\n",
    "For $w \\neq o$:\n",
    "$$ \\normalsize \\frac{\\partial }{\\partial u_{w}}J_{naive-softmax}(v_c;o;U) = \\hat{y}_{w}v_{c} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b6b1f1-9a69-4426-b5cf-e79dfef88265",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial }{\\partial u_{w}}J_{naive-softmax}(v_c;o;U) = \n",
    "\\left[\n",
    "\\begin{array}{ll}\n",
    "    v_{c}\\hat{y}_{w} & w \\neq o \\\\\n",
    "    v_{c}(\\hat{y}_{w} - 1) & w = o\n",
    "\\end{array}\n",
    "\\right .\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62fe5c3-6815-423a-a8b5-8f2e4920f081",
   "metadata": {},
   "source": [
    "# d."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b60d8d4-2b46-4740-bb47-23f8bf8a0131",
   "metadata": {},
   "source": [
    "**Question**: Compute the partial derivative of $J_{naive-softmax}(v_c, o, U)$ with respect to $U$. Please write your answer in terms of $\\frac{\\partial }{\\partial u_{1}}J(v_c, o, U), \\frac{\\partial }{\\partial u_{2}}J(v_c, o, U), \\frac{\\partial }{\\partial u_{|Vocab|}}J(v_c, o, U)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b29d9e-103e-4b8d-a8a4-89679108d004",
   "metadata": {},
   "source": [
    "**Answer**:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a24cc5c-63d4-49f5-b222-599ecaa9ff2c",
   "metadata": {},
   "source": [
    "# e."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fb83ed-49a8-4b56-b2b8-56831521234b",
   "metadata": {},
   "source": [
    "**Question**: The sigmoid function is given by Equation $\\normalsize \\sigma(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{1 + e^x}$\n",
    "Please compute the derivative of $\\sigma(x)$ with respect to $x$, where $x$ is a scalar.  \n",
    "Hint: you may want to write your answer in terms of $\\sigma(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368f29ed-3ec7-489d-95eb-5539dc2f8e21",
   "metadata": {},
   "source": [
    "**Answer**:  \n",
    "\n",
    "$\\normalsize \\sigma'(x) = \\frac{(e^x)'(1 + e^x) - e^x(1 + e^x)'}{(1 + e^x)^2} = \n",
    "\\frac{e^x(1 + e^x)}{(1 + e^x)^2} - \\frac{e^x}{(1 + e^x)}\\frac{e^x}{(1 + e^x)} =\n",
    "\\frac{e^x}{(1 + e^x)} - \\frac{e^x}{(1 + e^x)}\\frac{e^x}{(1 + e^x)} = \n",
    "\\sigma(x) - \\sigma(x)\\sigma(x) = \\sigma(x)(1 - \\sigma(x))$\n",
    "\n",
    "Remark: $\\normalsize \\sigma(-x) = 1 - \\sigma(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002cc323-e0f1-4fab-862a-406238e094a3",
   "metadata": {},
   "source": [
    "# f."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3dfd80-b4fd-4001-b9b6-be9267dd451c",
   "metadata": {},
   "source": [
    "**Question**: Now we shall consider the Negative Sampling loss, which is an alternative to the Naive Softmax loss. Assume that $K$ negative samples (words) are drawn from the vocabulary. For simplicity of notation we shall refer to them as $w_1, w_2; ..., w_K$ and their outside vectors as $u_1, ..., u_K$. For this question, assume that the $K$ negative samples are distinct. In other words, $i \\neq j$ implies $w_i \\neq w_j$ for $i, j \\in {1, ..., K}$. Note that $o \\neq {w_1, ..., w_K}$. For a center word $c$ and an outside word $o$, the negative sampling loss function is given by:\n",
    "$$\\normalsize J_{neg-sample}(v_c;o;U) = -log\\left(\\sigma(u^{T}_{o}v_{c})\\right) - \\sum_{k=1}^{K}{log\\left(\\sigma(-u^{T}_{k}v_{c})\\right)}$$  \n",
    "for a sample $w_1, ..., w_K$, where $\\sigma(·)$ is the sigmoid function.  \n",
    "Please repeat parts (b) and (c), computing the partial derivatives of $J_{neg-sample}$ with respect to $v_c$, with respect to $u_o$, and with respect to a negative sample $u_k$. Please write your answers in terms of the vectors $u_o$, $v_c$, and $u_k$, where $k \\in [1, K]$. After you've done this, describe with one sentence why this loss function is much more efficient to compute than the naive-softmax loss. Note, you should be able to use your solution to part (e) to help compute the necessary gradients here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcbdb44-be93-4ba0-82d7-9039be8f7d2d",
   "metadata": {},
   "source": [
    "**Answer**:  \n",
    "\n",
    "1. $\\normalsize \\frac{\\partial }{\\partial v_{c}}J_{neg-sample}(v_c;o;U)$:\n",
    "\n",
    "    $\\normalsize \\frac{\\partial }{\\partial v_{c}}\\left(-log\\left(\\sigma(u^{T}_{o}v_{c})\\right)\\right) = \n",
    "    -\\frac{1}{\\sigma(u^{T}_{o}v_{c})}\\sigma(u^{T}_{o}v_{c})(1 - \\sigma(u^{T}_{o}v_{c}))\\frac{\\partial }{\\partial v_{c}}u^{T}_{o}v_{c} = \n",
    "    (\\sigma(u^{T}_{o}v_{c}) - 1)u_{o}$  \n",
    "\n",
    "    $\\normalsize \\frac{\\partial }{\\partial v_{c}}\\left(-\\sum_{k=1}^{K}{log\\left(\\sigma(-u^{T}_{k}v_{c})\\right)}\\right) = \n",
    "    -\\sum_{k=1}^{K}{\\frac{\\partial }{\\partial v_{c}}log\\left(\\sigma(-u^{T}_{k}v_{c})\\right)} = \n",
    "    -\\sum_{k=1}^{K}{(-u_{k})(1 - \\sigma(-u^{T}_{k}v_{c}))} = \n",
    "    \\sum_{k=1}^{K}{u_{k}\\sigma(u^{T}_{k}v_{c})}$\n",
    "\n",
    "$$\\normalsize \\frac{\\partial }{\\partial v_{c}}J_{neg-sample}(v_c;o;U) = (\\sigma(u^{T}_{o}v_{c}) - 1)u_{o} + \\sum_{k=1}^{K}{u_{k}\\sigma(u^{T}_{k}v_{c})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a343cc3-0d96-406d-bd0d-31869cfb34d5",
   "metadata": {},
   "source": [
    "2. $\\normalsize \\frac{\\partial }{\\partial u_{k}}J_{neg-sample}(v_c;o;U)$:  \n",
    "    Note that $o \\notin {w_1; ... ;w_K}$ and where $k \\in [1;K]$  \n",
    "\n",
    "    $\\normalsize \\frac{\\partial }{\\partial u_{k}}\\left(-log\\left(\\sigma(u^{T}_{o}v_{c})\\right)\\right) = 0$  \n",
    "\n",
    "    $\\normalsize \\frac{\\partial }{\\partial u_{k}}\\left(-\\sum_{k=1}^{K}{log\\left(\\sigma(-u^{T}_{k}v_{c})\\right)}\\right) = \n",
    "    -{\\frac{\\partial }{\\partial u_{k}}log\\left(\\sigma(-u^{T}_{k}v_{c})\\right)} = \n",
    "    -{(-v_{c})(1 - \\sigma(-u^{T}_{k}v_{c}))} = \n",
    "    v_{c}\\sigma(u^{T}_{k}v_{c})$\n",
    "\n",
    "$$\\normalsize \\frac{\\partial }{\\partial u_{k}}J_{neg-sample}(v_c;o;U) = v_{c}\\sigma(u^{T}_{k}v_{c})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43069e8a-2c2c-4d74-a407-1be41fc608a3",
   "metadata": {},
   "source": [
    "2. $\\normalsize \\frac{\\partial }{\\partial u_{o}}J_{neg-sample}(v_c;o;U)$:  \n",
    "    Note that $o \\notin {w_1; ... ;w_K}$$  \n",
    "\n",
    "    $\\normalsize \\frac{\\partial }{\\partial u_{o}}\\left(-log\\left(\\sigma(u^{T}_{o}v_{c})\\right)\\right) = \n",
    "    -{\\frac{\\partial }{\\partial u_{o}}log\\left(\\sigma(u^{T}_{o}v_{c})\\right)} = \n",
    "    -(v_{c})(1 - \\sigma(u^{T}_{o}v_{c})) = \n",
    "    v_{c}(\\sigma(u^{T}_{o}v_{c}) - 1)$  \n",
    "\n",
    "    $\\normalsize \\frac{\\partial }{\\partial u_{o}}\\left(-\\sum_{k=1}^{K}{log\\left(\\sigma(-u^{T}_{k}v_{c})\\right)}\\right) = 0$\n",
    "\n",
    "$$\\normalsize \\frac{\\partial }{\\partial u_{o}}J_{neg-sample}(v_c;o;U) = v_{c}(\\sigma(u^{T}_{o}v_{c}) - 1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d2eacc-b060-4d5e-bce3-1598fbf8103e",
   "metadata": {},
   "source": [
    "So,   \n",
    "\\begin{align}\n",
    "\\frac{\\partial }{\\partial u_{k}}J_{neg-sample}(v_c;o;U) = \n",
    "\\left[\n",
    "\\begin{array}{ll}\n",
    "    v_{c}\\sigma(u^{T}_{k}v_{c}) & k \\neq o \\\\\n",
    "    v_{c}(\\sigma(u^{T}_{k}v_{c}) - 1) & k = o\n",
    "\\end{array}\n",
    "\\right .\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a789c8be-4fda-4187-93e8-c91cc9b59ab5",
   "metadata": {},
   "source": [
    "**Question**: *Why negative-sample loss function is much more efficient to compute than the naive-softmax loss?*  \n",
    "**Answer**:  \n",
    "Any update we do or evaluation of the naive-softmax function would take $O(|V|)$ time which if we recall is in the millions. For every training step using negative samples, instead of looping over the entire vocabulary, we can just sample several negative examples!\n",
    "\n",
    "$$ \\normalsize \\frac{\\partial }{\\partial v_{c}}J_{naive-softmax}(v_c;o;U) = U(\\hat{y} - y) $$\n",
    "\\begin{align}\n",
    "\\frac{\\partial }{\\partial u_{w}}J_{naive-softmax}(v_c;o;U) = \n",
    "\\left[\n",
    "\\begin{array}{ll}\n",
    "    v_{c}\\hat{y}_{w} & w \\neq o \\\\\n",
    "    v_{c}(\\hat{y}_{w} - 1) & w = o\n",
    "\\end{array}\n",
    "\\right .\n",
    "\\end{align}\n",
    "And  \n",
    "$$\\normalsize \\frac{\\partial }{\\partial v_{c}}J_{neg-sample}(v_c;o;U) = (\\sigma(u^{T}_{o}v_{c}) - 1)u_{o} + \\sum_{k=1}^{K}{u_{k}\\sigma(u^{T}_{k}v_{c})} $$\n",
    "\\begin{align}\n",
    "\\frac{\\partial }{\\partial u_{k}}J_{neg-sample}(v_c;o;U) = \n",
    "\\left[\n",
    "\\begin{array}{ll}\n",
    "    v_{c}\\sigma(u^{T}_{k}v_{c}) & k \\neq o \\\\\n",
    "    v_{c}(\\sigma(u^{T}_{k}v_{c}) - 1) & k = o\n",
    "\\end{array}\n",
    "\\right .\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1d2bf8-2df2-4262-9ea0-46200021190d",
   "metadata": {},
   "source": [
    "# e."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304c91d6-d23c-4ea4-aee8-e5edc964f110",
   "metadata": {},
   "source": [
    "**Question**: Now we will repeat the previous exercise, but without the assumption that the $K$ sampled words are distinct. Assume that $K$ negative samples (words) are drawn from the vocabulary. For simplicity of notation we shall refer to them as $w_1, w_2, ... , w_K$ and their outside vectors as $u_1, ..., u_K$. In this question, you may not assume that the words are distinct. In other words, $w_i = w_j$ may be true when $i \\neq j$ is true. Note that $o \\notin  {\\{w_1, ..., w_K\\}}$. For a center word $c$ and an outside word $o$, the negative sampling loss function is given by:\n",
    "$$\\normalsize J_{neg-sample}(v_c;o;U) = -log\\left(\\sigma(u^{T}_{o}v_{c})\\right) - \\sum_{k=1}^{K}{log\\left(\\sigma(-u^{T}_{k}v_{c})\\right)}$$\n",
    "for a sample $w_1, ..., w_K$, where $\\sigma(·)$ is the sigmoid function.  \n",
    "Compute the partial derivative of $J_{neg-sample}$ with respect to a negative sample $u_k$. Please write your answers in terms of the vectors $v_c$ and $u_k$, where $k \\in [1, K]$.  \n",
    "Hint: break up the sum in the loss function into two sums: a sum over all sampled words equal to $u_k$ and a sum over all sampled words not equal to $u_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a00d62-5d89-4102-8d1b-0074853632d5",
   "metadata": {},
   "source": [
    "**Answer**:  \n",
    "\n",
    "$\\normalsize \\frac{\\partial }{\\partial u_{k}}\\left(-log\\left(\\sigma(u^{T}_{o}v_{c})\\right)\\right) = 0$  \n",
    "\n",
    "$\\normalsize \\frac{\\partial }{\\partial u_{k}}\\left(-\\sum_{k=1}^{K}{log\\left(\\sigma(-u^{T}_{k}v_{c})\\right)}\\right) = \n",
    "-\\frac{\\partial }{\\partial u_{k}}\\left(\\sum_{l=1, w_l \\neq u_k}^{L}{log\\left(\\sigma(-u^{T}_{l}v_{c})\\right)} + \\sum_{n=1, w_n = u_k}^{N}{log\\left(\\sigma(-u^{T}_{n}v_{c})\\right)}\\right) = \n",
    "-N{\\frac{\\partial }{\\partial u_{k}}log\\left(\\sigma(-u^{T}_{k}v_{c})\\right)} = \n",
    "-N{(-v_{c})(1 - \\sigma(-u^{T}_{k}v_{c}))} = \n",
    "Nv_{c}\\sigma(u^{T}_{k}v_{c})$\n",
    "where $N$ is a number of times the exact word $w_k$ was sampled \n",
    "\n",
    "$$\\normalsize \\frac{\\partial }{\\partial u_{k}}J_{neg-sample}(v_c, o, U) = Nv_{c}\\sigma(u^{T}_{k}v_{c})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5fc8be-5561-485e-86af-3aa7b65147e5",
   "metadata": {},
   "source": [
    "# f."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfed0b4-e9c7-456f-8869-9fd8e3b28819",
   "metadata": {},
   "source": [
    "**Question**: Suppose the center word is $c = w_t$ and the context window is $[w_{t-m}, ..., w_{t-1}, w_t, w_{t+1}, ..., w_{t+m}]$, where $m$ is the context window size. Recall that for the skip-gram version of word2vec, the total loss for the context window is:\n",
    "\n",
    "$$\\normalsize J_{skip-gram}(v_c, w_{t-m}, ..., w_{t+m}, U) = \\sum_{\\substack{-m <= j <=m \\\\ j \\neq 0}}{J(v_c, w_{t+j}, U)}$$\n",
    "\n",
    "Here, $J(v_c, w_{t+j}, U)$ represents an arbitrary loss term for the center word $c = w_t$ and outside word $w_{t+j}$. $J(v_c, w_{t+j}, U)$ could be $J_{naive-softmax}(v_c, w_{t+j}, U)$ or $J_{neg-sample}(v_c, w_{t+j}, U)$, depending on your implementation.  \n",
    "Write down three partial derivatives:  \n",
    "(i) $\\normalsize\\frac{\\partial }{\\partial U}J_{skip-gram}(v_c, w_{t-m}, ..., w_{t+m}, U)$  \n",
    "(ii) $\\normalsize\\frac{\\partial }{\\partial v_c}J_{skip-gram}(v_c, w_{t-m}, ..., w_{t+m}, U)$   \n",
    "(iii) $\\normalsize\\frac{\\partial }{\\partial v_w}J_{skip-gram}(v_c, w_{t-m}, ..., w_{t+m}, U)$  when $w \\neq c$   \n",
    "Write your answers in terms of $\\frac{\\partial }{\\partial U}J(v_c, w_{t+j}, U)$ and $\\frac{\\partial }{\\partial v_c}J(v_c, w_{t+j}, U)$. This is very simple each solution should be one line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4926bfcf-2aa6-4056-b714-d79e474d1610",
   "metadata": {},
   "source": [
    "**Answer**:  \n",
    "\n",
    "$\\normalsize \\frac{\\partial }{\\partial U}J_{skip-gram}(v_c, w_{t-m}, ..., w_{t+m}, U) = \n",
    "\\frac{\\partial }{\\partial U}\\sum_{\\substack{-m <= j <=m \\\\ j \\neq 0}}{J(v_c, w_{t+j}, U)} =\n",
    "\\sum_{\\substack{-m <= j <=m \\\\ j \\neq 0}}{\\frac{\\partial }{\\partial U}J(v_c, w_{t+j}, U)}$  \n",
    "\n",
    "$\\normalsize \\frac{\\partial }{\\partial v_c}J_{skip-gram}(v_c, w_{t-m}, ..., w_{t+m}, U) = \n",
    "\\frac{\\partial }{\\partial v_c}\\sum_{\\substack{-m <= j <=m \\\\ j \\neq 0}}{J(v_c, w_{t+j}, U)} =\n",
    "\\sum_{\\substack{-m <= j <=m \\\\ j \\neq 0}}{\\frac{\\partial }{\\partial v_c}J(v_c, w_{t+j}, U)}$  \n",
    "\n",
    "$\\normalsize \\frac{\\partial }{\\partial v_w}J_{skip-gram}(v_c, w_{t-m}, ..., w_{t+m}, U) = \n",
    "\\frac{\\partial }{\\partial v_w}\\sum_{\\substack{-m <= j <=m \\\\ j \\neq 0}}{J(v_c, w_{t+j}, U)} = 0$,  when $w \\neq c$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd181ba-7fa2-4ba5-af21-098bccd28290",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
