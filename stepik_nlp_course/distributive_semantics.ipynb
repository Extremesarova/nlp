{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving tasks from Distributive semantics section of NLP Course from tepik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "\n",
    "def parse_array(s):\n",
    "    return np.array(ast.literal_eval(s))\n",
    "\n",
    "def read_array():\n",
    "    return parse_array(sys.stdin.readline())\n",
    "\n",
    "def write_array(arr):\n",
    "    print(repr(arr))\n",
    "\n",
    "def normalize_list(list):\n",
    "    return [l.tolist() if isinstance(l, np.ndarray) else l for l in list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generation of examples for training Word2Vec Skip Gram Negative Sampling\n",
    "\n",
    "We are training Word2Vec Skip Gram Negative Sampling with a window of a given width. For example, a window of size 5 implies that words that are no more than 2 positions to the left or right from the central word are considered positive examples. The center word is not counted as a context word.\n",
    "\n",
    "Write function, which generates training examples from the text. Every training example must look like a 3-element tuple $(CenterWord,CtxWord,Label)$, where $CenterWord∈N$ - token identifier in the middle of the window, $CtxWord∈N$ - identifier of adjacent token, $Label∈{0,1} - 1$ if $CtxWordCtxWord$ is positive and $0$, it is a negative example.\n",
    "\n",
    "Function should return the list with training examples.\n",
    "\n",
    "Arugment ns_rate sets the number of negative examples to generate for each positive example. When sampling negative words, it is usually not checked whether the word appears in the window. Thus, among negative examples, positive ones may appear.\n",
    "\n",
    "Input text was already tokenized and tokens were replaced with their identifiers.\n",
    "\n",
    "Tests are generated randomly, constraints:\n",
    "\n",
    " - len(text) < 20\n",
    " - window_size <= 11, нечётное\n",
    " - vocab_size < 100\n",
    " - ns_rate < 3\n",
    "Words have identifiers 0..vocab_size - 1 (as returns np.random.randint).\n",
    "\n",
    "NB, that -3 // 2 != -(3 // 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window(text, window_size):\n",
    "    for backward, current in enumerate(range(len(text)), start=0 - (window_size // 2)):\n",
    "        if backward < 0:\n",
    "            backward = 0\n",
    "        context = list(text[backward:current]) + list(text[current + 1:current + 1 + window_size // 2])\n",
    "        center = text[current]\n",
    "        yield center, context\n",
    "        \n",
    "def generate_w2v_sgns_samples(text, window_size, vocab_size, ns_rate):\n",
    "    \"\"\"\n",
    "    text - list of integer numbers - ids of tokens in text\n",
    "    window_size - odd integer - width of window\n",
    "    vocab_size - positive integer - number of tokens in vocabulary\n",
    "    ns_rate - positive integer - number of negative tokens to sample per one positive sample\n",
    "\n",
    "    returns list of training samples (CenterWord, CtxWord, Label)\n",
    "    \"\"\"\n",
    "    res = []\n",
    "\n",
    "    for center, context_values in get_window(text, window_size):\n",
    "        for context in context_values:\n",
    "            res.append([center, context, 1])\n",
    "            for n in range(ns_rate):\n",
    "                res.append([center, random.choice(np.array(range(0, vocab_size))), 0])\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 1], [1, 0, 0], [1, 1, 1], [1, 4, 0], [0, 1, 1], [0, 2, 0], [0, 1, 1], [0, 3, 0], [0, 0, 1], [0, 3, 0], [1, 1, 1], [1, 0, 0], [1, 0, 1], [1, 0, 0], [1, 0, 1], [1, 0, 0], [1, 0, 1], [1, 2, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [0, 3, 0], [0, 0, 1], [0, 0, 0], [0, 5, 1], [0, 2, 0], [0, 1, 1], [0, 5, 0], [0, 0, 1], [0, 0, 0], [0, 5, 1], [0, 1, 0], [0, 0, 1], [0, 4, 0], [5, 0, 1], [5, 4, 0], [5, 0, 1], [5, 3, 0], [5, 0, 1], [5, 1, 0], [5, 3, 1], [5, 1, 0], [0, 0, 1], [0, 4, 0], [0, 5, 1], [0, 5, 0], [0, 3, 1], [0, 5, 0], [0, 5, 1], [0, 4, 0], [3, 5, 1], [3, 1, 0], [3, 0, 1], [3, 0, 0], [3, 5, 1], [3, 5, 0], [3, 5, 1], [3, 4, 0], [5, 0, 1], [5, 4, 0], [5, 3, 1], [5, 5, 0], [5, 5, 1], [5, 3, 0], [5, 3, 1], [5, 2, 0], [5, 3, 1], [5, 0, 0], [5, 5, 1], [5, 3, 0], [5, 3, 1], [5, 4, 0], [5, 0, 1], [5, 5, 0], [3, 5, 1], [3, 0, 0], [3, 5, 1], [3, 0, 0], [3, 0, 1], [3, 5, 0], [3, 5, 1], [3, 2, 0], [0, 5, 1], [0, 4, 0], [0, 3, 1], [0, 1, 0], [0, 5, 1], [0, 5, 0], [0, 0, 1], [0, 0, 0], [5, 3, 1], [5, 4, 0], [5, 0, 1], [5, 3, 0], [5, 0, 1], [5, 1, 0], [5, 5, 1], [5, 0, 0], [0, 0, 1], [0, 0, 0], [0, 5, 1], [0, 0, 0], [0, 5, 1], [0, 4, 0], [0, 2, 1], [0, 5, 0], [5, 5, 1], [5, 5, 0], [5, 0, 1], [5, 1, 0], [5, 2, 1], [5, 5, 0], [5, 0, 1], [5, 4, 0], [2, 0, 1], [2, 1, 0], [2, 5, 1], [2, 1, 0], [2, 0, 1], [2, 1, 0], [2, 1, 1], [2, 4, 0], [0, 5, 1], [0, 5, 0], [0, 2, 1], [0, 2, 0], [0, 1, 1], [0, 2, 0], [0, 3, 1], [0, 3, 0], [1, 2, 1], [1, 3, 0], [1, 0, 1], [1, 0, 0], [1, 3, 1], [1, 0, 0], [3, 0, 1], [3, 5, 0], [3, 1, 1], [3, 3, 0]]\n"
     ]
    }
   ],
   "source": [
    "text = [1, 0, 1, 0, 0, 5, 0, 3, 5, 5, 3, 0, 5, 0, 5, 2, 0, 1, 3]\n",
    "window_size = 4\n",
    "vocab_size = 6\n",
    "ns_rate = 1\n",
    "\n",
    "result = generate_w2v_sgns_samples(text, window_size, vocab_size, ns_rate)\n",
    "\n",
    "write_array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training Word2Vec Skip Gram Negative Sampling for one example\n",
    "\n",
    "We are teaching Word2Vec Skip Gram Negative Sampling.\n",
    "\n",
    "Write a function that updates the model weights when receiving one training example in the format $(CenterWord, CtxWord, Label)$.\n",
    "\n",
    "During training, model predictions are calculated using the formula $P(CtxWord∣CenterWord)=σ(WCenterWord,:​⋅DCtxWord,:​)$\n",
    "\n",
    "The loss function is binary cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def update_w2v_weights(center_embeddings, context_embeddings, center_word, context_word, label, learning_rate):\n",
    "    \"\"\"\n",
    "    center_embeddings - VocabSize x EmbSize\n",
    "    context_embeddings - VocabSize x EmbSize\n",
    "    center_word - int - identifier of center word\n",
    "    context_word - int - identifier of context word\n",
    "    label - 1 if context_word is real, 0 if it is negative\n",
    "    learning_rate - float > 0 - size of gradient step\n",
    "    \"\"\"\n",
    "    sigma = sigmoid(np.dot(center_embeddings[center_word], context_embeddings[context_word]))\n",
    "    update_center = np.dot(learning_rate * (sigma - label), context_embeddings[context_word])\n",
    "    update_context = np.dot(learning_rate * (sigma - label), center_embeddings[center_word])\n",
    "    center_embeddings[center_word] -= update_center\n",
    "    context_embeddings[context_word] -= update_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_embeddings = [[0.3449417709491044, 0.6762047256081501, 0.9583446027893963],\n",
    "                     [0.6247126159157468, 0.22038323197740317, 0.29717611444948355],\n",
    "                     [0.9836099232994968, 0.3847689688960674, 0.033312247867206435],\n",
    "                     [0.4217704869846559, 0.0023859008971685025, 0.009686915033163657],\n",
    "                     [0.6933070658521228, 0.9705089533296152, 0.9189360293193337],\n",
    "                     [0.024858486425111903, 0.11331113152689753, 0.6492144300167894],\n",
    "                     [0.7861289466352543, 0.227319130535791, 0.8165251907260063],\n",
    "                     [0.7672181161105678, 0.04865001026002924, 0.07514404284170773]]\n",
    "context_embeddings = [[0.4628817426583818, 0.7747296319956671, 0.1374808935513827],\n",
    "                      [0.17026823169513283, 0.4094733988461122, 0.3175531656197459],\n",
    "                      [0.2910876746161247, 0.6340566555548147, 0.23158010794029804],\n",
    "                      [0.8449042648180852, 0.4796593509107806, 0.11278090182290745],\n",
    "                      [0.049097778744511156, 0.6254116250148337, 0.13038703647472905],\n",
    "                      [0.882545488649187, 0.6223076699449618, 0.1633041302523962],\n",
    "                      [0.6704032810194875, 0.941803340812521, 0.7358646489592193],\n",
    "                      [0.9875878745059805, 0.17935677165390562, 0.6798846454394736]]\n",
    "center_word = 2\n",
    "context_word = 5\n",
    "label = 0\n",
    "learning_rate = 0.342405260598321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3449417709491044, 0.6762047256081501, 0.9583446027893963], [0.6247126159157468, 0.22038323197740317, 0.29717611444948355], [0.7561584406822226, 0.22438652516534294, -0.008774836618697823], [0.4217704869846559, 0.0023859008971685025, 0.009686915033163657], [0.6933070658521228, 0.9705089533296152, 0.9189360293193337], [0.024858486425111903, 0.11331113152689753, 0.6492144300167894], [0.7861289466352543, 0.227319130535791, 0.8165251907260063], [0.7672181161105678, 0.04865001026002924, 0.07514404284170773]]\n",
      "[[0.4628817426583818, 0.7747296319956671, 0.1374808935513827], [0.17026823169513283, 0.4094733988461122, 0.3175531656197459], [0.2910876746161247, 0.6340566555548147, 0.23158010794029804], [0.8449042648180852, 0.4796593509107806, 0.11278090182290745], [0.049097778744511156, 0.6254116250148337, 0.13038703647472905], [0.6290474670186392, 0.5231442006778062, 0.15471882755224034], [0.6704032810194875, 0.941803340812521, 0.7358646489592193], [0.9875878745059805, 0.17935677165390562, 0.6798846454394736]]\n"
     ]
    }
   ],
   "source": [
    "update_w2v_weights(center_embeddings, context_embeddings,\n",
    "                   center_word, context_word, label, learning_rate)\n",
    "\n",
    "center_embeddings = normalize_list(center_embeddings)\n",
    "context_embeddings = normalize_list(context_embeddings)\n",
    "\n",
    "write_array(center_embeddings)\n",
    "write_array(context_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
